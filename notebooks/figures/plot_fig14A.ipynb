{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot PNG counts pre- / post-training\n",
    "\n",
    "Emergence of PNGs in hierarchical networks.\n",
    "\n",
    "**Results:**\n",
    "\n",
    "- Demonstrate emergence of polychronization in the network after training on datasets.\n",
    "- Plot PNG counts, considering simplest three-neuron HFB circuit\n",
    "- Show this with respect to the layers and network architectures\n",
    "\n",
    "**Dependencies:**\n",
    "\n",
    "---\n",
    "\n",
    "A) Significance testing:\n",
    "- PNG detection and significance testing for N3P2 & N4P2: both before and after network training\n",
    "- **These workflows are time-consuming to run**\n",
    "- N3P2 workflows (with and without the `--chkpt -1` argument):\n",
    "```bash\n",
    "./scripts/run_main_workflow.py experiments/n3p2/train_n3p2_lrate_0_04_181023 1 3 5 7 9 31 --rule significance -v\n",
    "```\n",
    "- N4P2 workflows (with and without the `--chkpt -1` argument):\n",
    "```bash\n",
    "./scripts/run_main_workflow.py experiments/n4p2/train_n4p2_lrate_0_02_181023 1 3 5 7 15 29 --rule significance -v\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "B) Compute PNG counts:\n",
    "\n",
    "i) For N3P2:\n",
    "```bash\n",
    "for arch in SEMI ALL; do\n",
    "    ./scripts/figures/compute_png_counts.py ./experiments/n3p2/train_n3p2_lrate_0_04_181023 $arch -v\n",
    "done\n",
    "```\n",
    "ii) and N4P2:\n",
    "```bash\n",
    "for arch in SEMI ALL; do\n",
    "    ./scripts/figures/compute_png_counts.py ./experiments/n4p2/train_n4p2_lrate_0_02_181023 $arch -v\n",
    "done\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Plots:**\n",
    "\n",
    "- Shape: (1, 2)\n",
    "- Columns: Dataset (N3P2, N4P2)\n",
    "- For each subplot, show:\n",
    "    - Untrained counts (FF + LAT + FB)\n",
    "    - Trained counts (FF + LAT) and (FF + LAT + FB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import pandas as pd\n",
    "\n",
    "from hsnn import viz\n",
    "from hsnn.utils import io\n",
    "\n",
    "pidx = pd.IndexSlice\n",
    "RESULTS_DIR = io.BASE_DIR / \"out/figures/detection\"\n",
    "OUTPUT_DIR = io.BASE_DIR / \"out/figures/fig14\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# Annotations\n",
    "class DataSet(Enum):\n",
    "    N3P2 = 1\n",
    "    N4P2 = 2\n",
    "\n",
    "\n",
    "ARCH_DESC_MAPPING = {\"FF\": \"FF\", \"SEMI\": \"FF+LAT\", \"ALL\": \"FF+LAT+FB\"}\n",
    "\n",
    "# Plotting\n",
    "prop_cycle = plt.rcParams[\"axes.prop_cycle\"]\n",
    "colors = prop_cycle.by_key()[\"color\"]\n",
    "\n",
    "\n",
    "viz.setup_journal_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Load PNG counts from both experiments (N3P2, N4P2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdirs = {DataSet.N3P2: \"n3p2\", DataSet.N4P2: \"n4p2\"}\n",
    "\n",
    "polygrps_counts: dict[DataSet, dict[str, dict[str, npt.NDArray[np.int_]]]] = {}\n",
    "for dataset, logdir in logdirs.items():\n",
    "    polygrps_counts[dataset] = io.load_pickle(RESULTS_DIR / logdir / \"png_counts.pkl\")\n",
    "    print(f\"Loaded '{dataset.name}' PNG counts from '{RESULTS_DIR / logdir}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Plot PNG counts\n",
    "\n",
    "- Untrained: `FF + LAT + FB`\n",
    "- Trained: `FF + LAT`, `FF + LAT + FB`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_counts(\n",
    "    num_polygrps: np.ndarray, layers: np.ndarray, ax: plt.Axes, label=None, **kwargs\n",
    "):\n",
    "    return ax.errorbar(\n",
    "        layers,\n",
    "        np.mean(num_polygrps, 0),\n",
    "        np.std(num_polygrps, 0) / np.sqrt(len(num_polygrps)),\n",
    "        label=label,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "\n",
    "arch_kwargs = {\n",
    "    \"SEMI\": {\"ls\": \"-\", \"marker\": \"o\"},\n",
    "    \"ALL\": {\"ls\": \"-\", \"marker\": \"o\"},\n",
    "    \"Untrained\": {\"ls\": \"--\", \"marker\": \"o\", \"markerfacecolor\": \"w\"},\n",
    "}\n",
    "yaxis_kwargs = {\n",
    "    DataSet.N3P2: {\n",
    "        \"ymax\": 8000,  # 3000\n",
    "        \"dy\": 2000,  # 500\n",
    "    },\n",
    "    DataSet.N4P2: {\n",
    "        \"ymax\": 8000,\n",
    "        \"dy\": 2000,\n",
    "    },\n",
    "}\n",
    "layers = [1, 2, 3, 4]\n",
    "width = 5.5\n",
    "height = 2.5\n",
    "\n",
    "f, axes = plt.subplots(1, 2, figsize=(width, height), sharey=True)\n",
    "for i, (dataset, counts_dict) in enumerate(polygrps_counts.items()):\n",
    "    ax: plt.Axes = axes[i]\n",
    "    for j, arch in enumerate([\"SEMI\", \"ALL\"]):\n",
    "        plot_counts(\n",
    "            counts_dict[arch][\"post\"],\n",
    "            layers,\n",
    "            ax=ax,\n",
    "            label=ARCH_DESC_MAPPING[arch],\n",
    "            color=colors[j],\n",
    "            **arch_kwargs[arch],\n",
    "        )\n",
    "        if arch == \"ALL\":\n",
    "            plot_counts(\n",
    "                counts_dict[arch][\"pre\"],\n",
    "                layers,\n",
    "                ax=ax,\n",
    "                label=\"Untrained\",\n",
    "                color=colors[1],\n",
    "                **arch_kwargs[\"Untrained\"],\n",
    "            )\n",
    "    ymax = yaxis_kwargs[dataset][\"ymax\"]\n",
    "    dy = yaxis_kwargs[dataset][\"dy\"]\n",
    "    ax.grid(color=\"lightgray\")\n",
    "    ax.set_xticks(layers)\n",
    "    ax.set_yticks(np.arange(0, ymax + dy, dy))\n",
    "    ax.set_title(dataset.name.upper(), pad=10, fontweight=\"bold\")\n",
    "    ax.set_xlabel(\"Layer\")\n",
    "    if i == 0:\n",
    "        ax.set_ylabel(\"# PNGs\")\n",
    "        # ax.legend(fontsize='x-small', loc=(0.52, 0.1))\n",
    "        ax.legend(fontsize=\"x-small\", loc=\"upper left\", frameon=True, framealpha=1)\n",
    "f.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "filedir = OUTPUT_DIR / \"fig_png_counts_datasets.pdf\"\n",
    "viz.save_figure(f, filedir, dpi=300, overwrite=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Report summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _relative_error(arr: np.ndarray) -> pd.DataFrame:\n",
    "    mean = np.mean(arr, axis=0)\n",
    "    std = np.std(arr, axis=0)  #  / np.sqrt(arr.shape[0])\n",
    "    rel_err = std / mean\n",
    "    return pd.DataFrame(\n",
    "        {\"mean\": mean, \"std\": std, \"rel_err\": rel_err, \"rel_err_pct\": rel_err * 100},\n",
    "        index=[int(i) for i in range(1, arr.shape[1] + 1)],\n",
    "    )\n",
    "\n",
    "\n",
    "rel_error_tables: dict[DataSet, pd.DataFrame] = {}\n",
    "for dataset, counts_dict in polygrps_counts.items():\n",
    "    rows = []\n",
    "    for arch, states in counts_dict.items():\n",
    "        for state, arr in states.items():\n",
    "            rel_df = _relative_error(arr)\n",
    "            rel_df[\"Architecture\"] = arch\n",
    "            rel_df[\"State\"] = state\n",
    "            rel_df[\"Layer\"] = rel_df.index\n",
    "            rows.append(rel_df.reset_index(drop=True))\n",
    "    rel_error_tables[dataset] = pd.concat(rows, ignore_index=True)\n",
    "\n",
    "for dataset, table in rel_error_tables.items():\n",
    "    rel_error_tables[dataset] = rel_error_tables[dataset].set_index(\n",
    "        [\"Architecture\", \"State\", \"Layer\"]\n",
    "    )\n",
    "    print(dataset)\n",
    "    display(\n",
    "        table[\n",
    "            [\"Architecture\", \"State\", \"Layer\", \"mean\", \"std\", \"rel_err\", \"rel_err_pct\"]\n",
    "        ]\n",
    "        .sort_values([\"Architecture\", \"State\", \"Layer\"])\n",
    "        .reset_index(drop=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_rel_err_pct_mean(\n",
    "    dataset: DataSet,\n",
    ") -> pd.DataFrame:\n",
    "    return (\n",
    "        rel_error_tables[dataset]\n",
    "        .groupby([\"Architecture\", \"State\"])\n",
    "        .agg({\"rel_err_pct\": (\"mean\", \"max\", \"min\")})\n",
    "    )\n",
    "\n",
    "\n",
    "for dataset in DataSet:\n",
    "    print(dataset)\n",
    "    display(report_rel_err_pct_mean(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = report_rel_err_pct_mean(DataSet.N4P2)\n",
    "print(f\"Mean relative error: {df['rel_err_pct']['mean'].mean():.1f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Statistical test**\n",
    "\n",
    "Determine whether the number of counts for the network with feedback is significantly greater\n",
    "- Two-Way ANOVA (Analysis of Variance) statistical test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Convert counts_dict to the data format for ANOVA\n",
    "dataset = DataSet.N3P2\n",
    "layers = [\"L2\", \"L3\", \"L4\"]\n",
    "n_seeds, n_layers = 3, 3\n",
    "\n",
    "data = {\n",
    "    \"PNG_Count\": [],\n",
    "    \"Architecture\": [],\n",
    "    \"State\": [],\n",
    "    \"Layer\": [],\n",
    "}\n",
    "\n",
    "for arch, states in polygrps_counts[dataset].items():\n",
    "    for state, arr in states.items():\n",
    "        # arr shape: (n_seeds, n_layers)\n",
    "        for seed_idx in range(n_seeds):\n",
    "            for layer_idx in range(n_layers):\n",
    "                data[\"PNG_Count\"].append(arr[seed_idx, layer_idx + 1])\n",
    "                data[\"Architecture\"].append(arch)\n",
    "                data[\"State\"].append(state)\n",
    "                data[\"Layer\"].append(layers[layer_idx])\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df = df[df[\"State\"] == \"post\"].drop(\"State\", axis=1)\n",
    "df = df.sort_values([\"Architecture\", \"Layer\"], ascending=[False, True]).reset_index(\n",
    "    drop=True\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define the model\n",
    "# The formula \"PNG_Count ~ C(Architecture) * C(Layer)\" automatically includes:\n",
    "# - Main Effect of Architecture\n",
    "# - Main Effect of Layer\n",
    "# - The Interaction (Architecture:Layer)\n",
    "model = ols(\"PNG_Count ~ C(Architecture) * C(Layer)\", data=df).fit()\n",
    "\n",
    "# 3. Generate the ANOVA table\n",
    "# typ=2 is standard for this kind of design\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "anova_table.to_csv(RESULTS_DIR / f\"statistical_test_{dataset.name}.csv\")\n",
    "print(anova_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_df = pd.concat(\n",
    "    {\n",
    "        (arch, state): pd.DataFrame(\n",
    "            arr, columns=[int(i) for i in range(1, arr.shape[1] + 1)]\n",
    "        )\n",
    "        for arch, states in counts_dict.items()\n",
    "        for state, arr in states.items()\n",
    "    }\n",
    ")\n",
    "counts_df.index.names = [\"Architecture\", \"State\", \"Trial\"]\n",
    "counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_stats = counts_df.groupby(level=[\"Architecture\", \"State\"]).agg([\"mean\", \"sem\"])\n",
    "counts_stats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hsnn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
