{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot onset timings and regression\n",
    "\n",
    "PNG onset timing, timing precision, and feature selectivity for three-sided shapes (N3P2).\n",
    "\n",
    "**Dependencies:**\n",
    "\n",
    "Spike recordings, PNG detection and significance testing:\n",
    "- Note that recorded spike trains, PNG significance testing are non-deterministic: **results may differ from manuscript**\n",
    "- Takes ~1 hr (AMD Ryzen 9 5900X, 64GB RAM) to run the entire workflow\n",
    "```bash\n",
    "./scripts/run_main_workflow.py experiments/n3p2/train_n3p2_lrate_0_04_181023 31 --layers 4 --configfile config/workflow/config_onsets.yaml --chkpt -1 --subdir onsets --rule significance -v\n",
    "```\n",
    "\n",
    "**Plots:**\n",
    "\n",
    "PNGs that are selective to left- right- and top-convex feature elements:\n",
    "- Empirical distribution of PNG onset times\n",
    "- Timing dispersion (standard deviation) versus mean onset time computed across repetitions for each PNG-side pairing\n",
    "- Mean onset time versus convex-boundary selectivity (F1 score) for each PNG-side pairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import scipy.stats as spstats\n",
    "from scipy.stats import linregress\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import hsnn.analysis.png.db as polydb\n",
    "from hsnn import analysis, simulation, utils, viz\n",
    "from hsnn.analysis.png import stats\n",
    "from hsnn.utils import handler\n",
    "\n",
    "pidx = pd.IndexSlice\n",
    "OUTPUT_DIR = utils.io.BASE_DIR / \"out/figures/fig19\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# Plotting\n",
    "prop_cycle = plt.rcParams[\"axes.prop_cycle\"]\n",
    "colors = prop_cycle.by_key()[\"color\"]\n",
    "\n",
    "viz.setup_journal_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Select experiment\n",
    "\n",
    "Load representative trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = \"n3p2/train_n3p2_lrate_0_04_181023\"\n",
    "\n",
    "expt = handler.ExperimentHandler(logdir)\n",
    "dataset_name = Path(logdir).parent.name\n",
    "print(f\"Target dataset: {dataset_name}\")\n",
    "# Get relevant, representative Trials\n",
    "df = expt.get_summary(-1)\n",
    "closest_trials = expt.index_to_dir[handler.get_closest_samples(df)]\n",
    "closest_trials.drop([(0, 0), (0, 20), (20, 0)], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Load data\n",
    "\n",
    "Including Trial, spike records, HFB DB, detected PNGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect TrialsDict\n",
    "model_type = \"ALL\"\n",
    "analysis_type = \"onsets\"\n",
    "states = (\"post\",)\n",
    "\n",
    "print(f\"Selected network type: '{model_type}'; results: '{analysis_type}'\\n\")\n",
    "offset = 0.0 if analysis_type == \"onsets\" else 50.0\n",
    "trials_dict = expt.metadata.get_trials_dict(model_type)\n",
    "\n",
    "# View relevant trials\n",
    "trial_names = trials_dict[analysis_type]\n",
    "print(\"# Trials to analyse:\")\n",
    "pprint([expt[trial_name] for trial_name in trial_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_id = \"TrainSNN_eb0d4_00031\"\n",
    "state = \"post\"\n",
    "subdir = \"onsets\"\n",
    "num_reps = 10\n",
    "\n",
    "# Get representative Trial\n",
    "trial = expt[trial_id]\n",
    "print(trial)\n",
    "\n",
    "# Get relevant spike records\n",
    "result = handler.load_results(trial, state, subdir=subdir)[state].sel(\n",
    "    rep=range(num_reps)\n",
    ")\n",
    "duration = result.item(0).duration - offset\n",
    "assert len(result.rep) == 10\n",
    "print(f\"\\nduration={duration}; offset={offset}; num_reps={num_reps}\")\n",
    "\n",
    "# Load imageset and labels\n",
    "cfg = trial.config\n",
    "imageset, labels = utils.io.get_dataset(\n",
    "    cfg[\"training\"][\"data\"], return_annotations=True\n",
    ")\n",
    "\n",
    "# Get relevant HFB database\n",
    "database = handler.load_detections(trial, state, subdir=subdir)[state]\n",
    "print(f\"Loaded HFB database '{database.path}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Restore Network and get refined PNGs\n",
    "For inspection of ground-truth axonal conduction delays, weights, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = simulation.Simulator.from_config(cfg)\n",
    "if state == \"post\":\n",
    "    sim.restore(trial.checkpoints[-1].store_path)\n",
    "    print(f\"Restoring from checkpoint '{trial.checkpoints[-1].store_path}'\")\n",
    "syn_params: pd.DataFrame = sim.network.get_syn_params()\n",
    "syn_params = syn_params.loc[pidx[slice(None), (\"FF\", \"E2E\")], :].sort_index(\n",
    "    inplace=False\n",
    ")\n",
    "\n",
    "# Get detected PNGs (final layer)\n",
    "polygrps = polydb.get_polygrps(database, syn_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Get inference / detection metrics\n",
    "\n",
    "Including the following:\n",
    "- Single-neuron specific information\n",
    "- PNG performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get single-neuron specific information measures for a target per side (convex)\n",
    "target = 1\n",
    "\n",
    "# Firing rates of EXC neuron across last two layers\n",
    "rates_array = analysis.infer_rates(\n",
    "    result.sel(layer=[3, 4], nrn_cls=\"EXC\"), duration, offset\n",
    ")\n",
    "\n",
    "specific_measures: dict[\n",
    "    str, dict[str, pd.DataFrame]\n",
    "] = {}  # layer -> side -> nrn measures\n",
    "for layer in tqdm([3, 4]):\n",
    "    specific_measures[layer] = analysis.get_specific_measures_side(\n",
    "        rates_array.sel(layer=layer), labels, target=target\n",
    "    )\n",
    "\n",
    "# Get PNG performance metrics per side\n",
    "occ_array = stats.get_occurrences_array(\n",
    "    polygrps, num_reps, len(imageset), index=1, duration=duration, offset=offset\n",
    ")\n",
    "metrics_side = stats.get_metrics_side(occ_array, labels, target)  # side -> PNG metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Onsets analysis\n",
    "\n",
    "**Gather onsets across PNGs with recorded scores per side**\n",
    "\n",
    "- Gather recordings across all side-specific measures\n",
    "- Drop all PNGs without an F1 score for any side\n",
    "- Get flattened onsets across all retained PNGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scored_onsets(\n",
    "    onsets_array: xr.DataArray, metrics: pd.DataFrame, img: int | None = None\n",
    ") -> xr.DataArray:\n",
    "    _onsets_array = onsets_array if img is None else onsets_array.sel(img=img)\n",
    "    mask = np.isin(_onsets_array[\"png\"], metrics.index)\n",
    "    png_sel = _onsets_array[\"png\"].values[mask]\n",
    "    return _onsets_array.sel(png=png_sel)\n",
    "\n",
    "\n",
    "def get_onset_statistics(\n",
    "    onsets_series: pd.Series, agg: list[str], min_reps: int = 3\n",
    ") -> pd.Series:\n",
    "    assert set(onsets_series.index.names) == {\"side\", \"png\", \"rep\"}\n",
    "    assert isinstance(onsets_series, pd.Series)\n",
    "    onsets_series_ = onsets_series.groupby([\"side\", \"png\"]).filter(\n",
    "        lambda x: len(x) >= min_reps\n",
    "    )\n",
    "    return onsets_series_.groupby([\"side\", \"png\"]).agg(agg)\n",
    "\n",
    "\n",
    "sides = labels.drop(\"image_id\", axis=1).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onsets_array = stats.get_onsets_array(polygrps, num_reps, len(imageset), null_img=5)\n",
    "\n",
    "onsets_side = {}\n",
    "for side in sides:\n",
    "    onsets_array_sel = get_scored_onsets(\n",
    "        onsets_array, metrics_side[side], img=utils.get_unique_id(labels, side)\n",
    "    )\n",
    "    onsets_series = (\n",
    "        onsets_array_sel.stack(sample=(\"png\", \"rep\")).dropna(\"sample\").to_pandas()\n",
    "    )\n",
    "    onsets_series.name = \"onset\"\n",
    "\n",
    "    onsets_side[side] = pd.merge(\n",
    "        onsets_series,\n",
    "        metrics_side[side][\"score\"],\n",
    "        how=\"left\",\n",
    "        left_on=\"png\",\n",
    "        right_index=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0, 1, 21)\n",
    "\n",
    "onsets_concat: pd.DataFrame = pd.concat(onsets_side, names=[\"side\"])\n",
    "onsets_df = onsets_concat.reset_index(drop=True)\n",
    "onsets_df[\"score_bin\"] = pd.cut(onsets_df[\"score\"], bins=bins, include_lowest=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**i) Plot histogram and mean vs stdev**\n",
    "\n",
    "- Show distribution of individual onset times: peak at short times\n",
    "- Get mean w.r.t. each individual PNG across all reps (reps > occ_thr) and stdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_av_stdev(\n",
    "    onset_means: np.ndarray, onset_stdevs: np.ndarray, axes: plt.Axes\n",
    ") -> plt.Axes:\n",
    "    axes.scatter(onset_means, onset_stdevs, s=3)\n",
    "    axes.set_axisbelow(True)\n",
    "    axes.grid()\n",
    "    axes.set_xticks(np.arange(0, 250, 50))\n",
    "    axes.set_xlim([0, 200])\n",
    "    axes.set_yticks(np.arange(0, 150, 50))\n",
    "    axes.set_ylim([0, 100])\n",
    "    axes.set_xlabel(\"Mean onset [ms]\")\n",
    "    axes.set_ylabel(\"Dispersion [ms]\")\n",
    "    return axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get mean and stdev of onsets per PNG-side paring, for PNGs with at least 3 reps\n",
    "onset_png_stats = get_onset_statistics(onsets_concat[\"onset\"], [\"mean\", \"std\"])\n",
    "onset_png_stats.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of onsets across PNGs and mean vs stdev of onsets per PNG\n",
    "num_bins = 60\n",
    "ymax = 0.04\n",
    "dy = 0.02\n",
    "\n",
    "f, axes = plt.subplots(1, 2, figsize=(5.5, 5.5 / 3))\n",
    "\n",
    "ax: plt.Axes = axes[0]\n",
    "ax.hist(\n",
    "    onsets_df[\"onset\"].values, num_bins, density=True, edgecolor=\"C0\", linewidth=0.5\n",
    ")\n",
    "ax.set_axisbelow(True)\n",
    "ax.grid()\n",
    "ax.set_xlim([0, 200])\n",
    "ax.set_ylim([0, ymax])\n",
    "ax.set_yticks(np.arange(0, ymax + dy, dy))\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "ax.set_xlabel(\"Onset [ms]\")\n",
    "\n",
    "ax = plot_av_stdev(onset_png_stats[\"mean\"], onset_png_stats[\"std\"], axes=axes[-1])\n",
    "\n",
    "f.tight_layout()\n",
    "\n",
    "filedir = OUTPUT_DIR / f\"fig_png_onset_distr_n3p2_{model_type}.pdf\"\n",
    "viz.save_figure(f, filedir, overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Panel A) stats\n",
    "counts, bin_edges = np.histogram(onsets_df[\"onset\"], num_bins)\n",
    "\n",
    "arg_max = counts.argmax()\n",
    "print(f\"Mode: {np.mean(bin_edges[arg_max : arg_max + 2]):.1f} ms\")\n",
    "\n",
    "cpr = counts.cumsum() / counts.cumsum()[-1]\n",
    "arg_mid = np.abs(0.5 - cpr).argmin()\n",
    "print(f\"Median: {bin_edges[arg_mid]:.1f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Panel B) select stats\n",
    "mask = onset_png_stats[\"mean\"].between(0, 30)\n",
    "onset_png_stats[\"mean\"][mask].mean(), onset_png_stats[\"mean\"][mask].std()\n",
    "print(\n",
    "    f\"Mean of mean onsets between 0 and 30 ms: {onset_png_stats['mean'][mask].mean():.1f} ms\"\n",
    ")\n",
    "print(\n",
    "    f\"Standard deviation of mean onsets between 0 and 30 ms: {onset_png_stats['mean'][mask].std():.1f} ms\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ii) Plot mean onset vs. F1 score**\n",
    "\n",
    "- Demonstrate significant trend: earlier onset time with higher F1 score\n",
    "- Do linear regression\n",
    "- Ensure data points are aligned with the above scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get onset vs score per PNG-side pairing (for PNGs with at least 3 reps)\n",
    "mask = onsets_concat.index.droplevel(\"rep\").isin(onset_png_stats.index)\n",
    "png_shape_onset_score = (\n",
    "    onsets_concat.loc[mask]\n",
    "    .groupby([\"side\", \"png\"])\n",
    "    .agg(onset=(\"onset\", \"mean\"), score=(\"score\", \"mean\"))\n",
    ")\n",
    "png_shape_df = png_shape_onset_score.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regress against individual data points\n",
    "xs = png_shape_df[\"score\"].values\n",
    "ys = png_shape_df[\"onset\"].values\n",
    "\n",
    "slope, intercept, r_value, p_value, std_err = linregress(xs, ys)\n",
    "xs_est = np.array([0, 1])\n",
    "ys_est = slope * xs_est + intercept\n",
    "\n",
    "print(\n",
    "    f\"Gradient: {slope:.1f}; intercept: {intercept:.1f}; r^2: {r_value**2:.3f}; \"\n",
    "    f\"p_value: {p_value:0.0e}; std_err: {std_err:.1f}; num pts: {len(xs)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 5.5\n",
    "\n",
    "axes: plt.Axes\n",
    "f, axes = plt.subplots(figsize=(width, width * 1 / 2))\n",
    "axes.scatter(\n",
    "    png_shape_df[\"score\"].values,\n",
    "    png_shape_df[\"onset\"].values,\n",
    "    s=8,\n",
    "    alpha=0.6,\n",
    "    color=\"C0\",\n",
    ")\n",
    "\n",
    "# Regression line and 95% CI band\n",
    "xs_line = np.linspace(0, 1.0, 200)\n",
    "ys_line = slope * xs_line + intercept\n",
    "n = len(xs)\n",
    "xbar = xs.mean()\n",
    "ssxx = np.sum((xs - xbar) ** 2)\n",
    "residuals = ys - (slope * xs + intercept)\n",
    "syx = np.sqrt(np.sum(residuals**2) / (n - 2))\n",
    "t_crit = spstats.t.ppf(0.975, df=n - 2)\n",
    "se_mean = syx * np.sqrt(1 / n + (xs_line - xbar) ** 2 / ssxx)\n",
    "ci_lower = ys_line - t_crit * se_mean\n",
    "ci_upper = ys_line + t_crit * se_mean\n",
    "\n",
    "axes.plot(xs_line, ys_line, \"C1-\", linewidth=1.5)\n",
    "axes.fill_between(xs_line, ci_lower, ci_upper, color=\"C1\", alpha=0.2, linewidth=0)\n",
    "\n",
    "# Regression statistics\n",
    "beta1_per_0_1 = slope * 0.1\n",
    "ci_lower_0_1 = (slope - (spstats.t.ppf(1 - 0.025, df=len(xs) - 2) * std_err)) * 0.1\n",
    "ci_upper_0_1 = (slope + (spstats.t.ppf(1 - 0.025, df=len(xs) - 2) * std_err)) * 0.1\n",
    "p_str = (\n",
    "    f\"p < 10$^{{{int(np.floor(np.log10(p_value)))}}}$\"\n",
    "    if p_value < 0.001\n",
    "    else f\"p = {p_value:.3f}\"\n",
    ")\n",
    "axes.text(\n",
    "    0.98,\n",
    "    0.95,\n",
    "    \"Linear fit: $\\\\beta_1$ = \"\n",
    "    f\"{beta1_per_0_1:.1f} ms per 0.1 F1 \"\n",
    "    f\"(95% CI {ci_lower_0_1:.1f}, {ci_upper_0_1:.1f})\",\n",
    "    transform=axes.transAxes,\n",
    "    ha=\"right\",\n",
    "    va=\"top\",\n",
    "    fontsize=\"small\",\n",
    "    bbox=dict(facecolor=\"white\", alpha=1),\n",
    ")\n",
    "\n",
    "axes.set_axisbelow(True)\n",
    "axes.grid()\n",
    "axes.set_xlim([0, 1.0])\n",
    "axes.set_ylim([0, 200])\n",
    "axes.set_xticks(np.arange(0, 1.2, 0.2))\n",
    "axes.set_yticks(np.arange(0, 220, 40))\n",
    "axes.set_ylabel(\"Mean onset [ms]\")\n",
    "axes.set_xlabel(\"F1 score\")\n",
    "f.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "filedir = OUTPUT_DIR / f\"fig_png_onset_f1_n3p2_{model_type}.pdf\"\n",
    "viz.save_figure(f, filedir, overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate 95% Confidence Interval\n",
    "n = len(xs)\n",
    "ci_interval = 0.95\n",
    "t_crit = spstats.t.ppf(\n",
    "    1 - (1 - ci_interval) / 2, df=n - 2\n",
    ")  # Two-tailed t-score for 95%\n",
    "ci_margin = t_crit * std_err\n",
    "\n",
    "ci_lower = slope - ci_margin\n",
    "ci_upper = slope + ci_margin\n",
    "\n",
    "print(f\"Report: Slope = {slope:.2f} Â± {std_err:.2f} (SE)\")\n",
    "print(f\"Report: Slope = {slope:.2f} [95% CI: {ci_lower:.2f}, {ci_upper:.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print regression metadata\n",
    "regression_meta = {\n",
    "    \"model_type\": model_type,\n",
    "    \"analysis_type\": analysis_type,\n",
    "    \"n_samples\": int(len(xs)),\n",
    "    \"slope\": float(slope),\n",
    "    \"intercept\": float(intercept),\n",
    "    \"r_value\": float(r_value),\n",
    "    \"r_squared\": float(r_value**2),\n",
    "    \"p_value\": float(p_value),\n",
    "    \"std_err\": float(std_err),\n",
    "    \"ci_interval\": float(ci_interval),\n",
    "    \"ci_lower\": float(ci_lower),\n",
    "    \"ci_upper\": float(ci_upper),\n",
    "}\n",
    "\n",
    "meta_path = OUTPUT_DIR / f\"regression_onset_f1_n3p2_{model_type}.json\"\n",
    "pd.Series(regression_meta).to_json(meta_path, indent=4)\n",
    "print(f\"Saved regression metadata -> {meta_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hsnn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
