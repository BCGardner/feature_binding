{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supplementary: Side-resolved information analysis\n",
    "\n",
    "Single-neuron information analysis and informative-neuron counts for convex/concave boundaries at each N3P2/N4P2 object side.\n",
    "\n",
    "- This plots S3 and S4 Figs.\n",
    "- This is the (FF + LAT + FB) network architecture.\n",
    "\n",
    "**Dependencies:**\n",
    "\n",
    "---\n",
    "\n",
    "A) Gather inference spike recordings:\n",
    "- Inference spike recordings for N3P2 & N4P2: both before and after network training.\n",
    "- Artifacts that have already been generated from a previous workflow run will be skipped.\n",
    "- Depends on N3P2 workflows (with and without the `--chkpt -1` argument):\n",
    "    - `./scripts/run_main_workflow.py experiments/n3p2/train_n3p2_lrate_0_04_181023 0 1 3 4 5 7 8 9 31 --rule inference -v`\n",
    "- Depends on N4P2 workflows (with and without the `--chkpt -1` argument):\n",
    "    - `./scripts/run_main_workflow.py experiments/n4p2/train_n4p2_lrate_0_02_181023 0 1 3 4 5 7 12 15 29 --rule inference -v`\n",
    "\n",
    "---\n",
    "\n",
    "B) Compute information measures:\n",
    "\n",
    "- Also run these with the argument `--target 0` for concave selectivity.\n",
    "\n",
    "i) For N3P2:\n",
    "```bash\n",
    "for side in top left right; do\n",
    "    ./scripts/figures/compute_information.py ./experiments/n3p2/train_n3p2_lrate_0_04_181023 ALL \\\n",
    "        --side $side  --target 1 --output_dir ./out/figures/supplementary/information -v\n",
    "done\n",
    "```\n",
    "ii) and N4P2:\n",
    "```bash\n",
    "for side in top left bottom right; do\n",
    "    ./scripts/figures/compute_information.py ./experiments/n4p2/train_n4p2_lrate_0_02_181023 ALL \\\n",
    "        --side $side  --target 1 --output_dir ./out/figures/supplementary/information -v\n",
    "done\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Plots**\n",
    "\n",
    "- N3P2 and N4P2 Figs\n",
    "- Panel A: rank-order single neuron information curves\n",
    "- Panel B: number of selective neurons (exceeding 2/3 threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Literal\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import pandas as pd\n",
    "\n",
    "from hsnn import viz\n",
    "from hsnn.utils import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Configuration === #\n",
    "RESULTS_DIR = io.BASE_DIR / \"out/figures/supplementary/information\"\n",
    "OUTPUT_DIR = io.BASE_DIR / \"out/figures/supplementary/figs_S4_S5\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Dataset-specific sides\n",
    "SIDES_N3P2 = [\"top\", \"left\", \"right\"]\n",
    "SIDES_N4P2 = [\"top\", \"left\", \"bottom\", \"right\"]\n",
    "\n",
    "# Conformations to plot\n",
    "CONFORMATIONS = [\"convex\", \"concave\"]\n",
    "\n",
    "# Architecture to use (FF+LAT+FB)\n",
    "ARCH = \"ALL\"\n",
    "\n",
    "# Information threshold guide line (optional)\n",
    "INFO_THRESHOLD = 2 / 3\n",
    "\n",
    "# Setup journal environment for publication-quality figures\n",
    "viz.setup_journal_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Data Loading Functions\n",
    "\n",
    "Load precomputed information measures following `compute_information.py` naming conventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_expected_filename(side: str, conformation: str, noise: int = 0) -> str:\n",
    "    \"\"\"Generate expected filename following compute_information.py conventions.\n",
    "\n",
    "    The script saves files as: information_{side}_{conformation}_noise_{noise}.pkl\n",
    "    \"\"\"\n",
    "    fname = io.formatted_name(f\"information_{side}_{conformation}\", \"pkl\", noise=noise)\n",
    "    return fname\n",
    "\n",
    "\n",
    "def load_side_measures(\n",
    "    dataset_dir: Path,\n",
    "    side: str,\n",
    "    conformation: str,\n",
    "    arch: str = \"ALL\",\n",
    "    noise: int = 0,\n",
    ") -> dict[str, npt.NDArray[np.floating]]:\n",
    "    \"\"\"Load precomputed information measures for a specific side and conformation.\n",
    "\n",
    "    Args:\n",
    "        dataset_dir: Directory containing precomputed results (e.g., RESULTS_DIR/n3p2)\n",
    "        side: Boundary side (e.g., 'left', 'top', 'right', 'bottom')\n",
    "        conformation: Target conformation ('convex' or 'concave')\n",
    "        arch: Network architecture key (default: 'ALL' for FF+LAT+FB)\n",
    "        noise: Noise amplitude (default: 0)\n",
    "\n",
    "    Returns:\n",
    "        Dict with 'pre' and 'post' arrays of shape (num_trials, num_nrns)\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the expected file does not exist\n",
    "        KeyError: If the architecture key is not found in the data\n",
    "    \"\"\"\n",
    "    fname = get_expected_filename(side, conformation, noise)\n",
    "    fpath = dataset_dir / fname\n",
    "\n",
    "    if not fpath.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Expected file not found: {fpath}\\n\"\n",
    "            f\"Please run: scripts/figures/compute_information.py with --side {side} --target {1 if conformation == 'convex' else 0}\"\n",
    "        )\n",
    "\n",
    "    data = io.load_pickle(fpath)\n",
    "\n",
    "    if arch not in data:\n",
    "        available = list(data.keys())\n",
    "        raise KeyError(\n",
    "            f\"Architecture '{arch}' not found in {fpath}. Available: {available}\"\n",
    "        )\n",
    "\n",
    "    return data[arch]\n",
    "\n",
    "\n",
    "def load_dataset_measures(\n",
    "    dataset_name: Literal[\"n3p2\", \"n4p2\"],\n",
    "    arch: str = \"ALL\",\n",
    "    noise: int = 0,\n",
    ") -> dict[str, dict[str, dict[str, npt.NDArray[np.floating]]]]:\n",
    "    \"\"\"Load all side and conformation measures for a dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset_name: Dataset identifier ('n3p2' or 'n4p2')\n",
    "        arch: Network architecture key\n",
    "        noise: Noise amplitude\n",
    "\n",
    "    Returns:\n",
    "        Nested dict: measures[side][conformation] -> {'pre': array, 'post': array}\n",
    "    \"\"\"\n",
    "    dataset_dir = RESULTS_DIR / dataset_name\n",
    "    sides = SIDES_N3P2 if dataset_name == \"n3p2\" else SIDES_N4P2\n",
    "\n",
    "    if not dataset_dir.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Dataset directory not found: {dataset_dir}\\n\"\n",
    "            f\"Please run scripts/figures/compute_information.py for {dataset_name}\"\n",
    "        )\n",
    "\n",
    "    measures: dict[str, dict[str, dict[str, npt.NDArray]]] = {}\n",
    "    loaded_files: list[str] = []\n",
    "\n",
    "    for side in sides:\n",
    "        measures[side] = {}\n",
    "        for conformation in CONFORMATIONS:\n",
    "            data = load_side_measures(dataset_dir, side, conformation, arch, noise)\n",
    "            measures[side][conformation] = data\n",
    "            fname = get_expected_filename(side, conformation, noise)\n",
    "            loaded_files.append(fname)\n",
    "\n",
    "    # Print loading summary\n",
    "    print(f\"\\n=== Loaded {dataset_name.upper()} measures ===\")\n",
    "    print(f\"Directory: {dataset_dir}\")\n",
    "    print(f\"Architecture: {arch}\")\n",
    "    print(f\"Files loaded: {len(loaded_files)}\")\n",
    "    for fname in loaded_files:\n",
    "        print(f\"  - {fname}\")\n",
    "\n",
    "    # Print shape info from first entry\n",
    "    first_side = sides[0]\n",
    "    first_conf = CONFORMATIONS[0]\n",
    "    for state in [\"pre\", \"post\"]:\n",
    "        shape = measures[first_side][first_conf][state].shape\n",
    "        print(f\"Shape ({state}): {shape}\")\n",
    "\n",
    "    return measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Plotting Functions\n",
    "\n",
    "Match styling from `plot_information.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rank_curve(\n",
    "    ax: plt.Axes,\n",
    "    measures: dict[str, npt.NDArray[np.floating]],\n",
    "    show_legend: bool = False,\n",
    ") -> None:\n",
    "    \"\"\"Plot rank-ordered information curves for pre and post training.\n",
    "\n",
    "    Args:\n",
    "        ax: Matplotlib axes to plot on\n",
    "        measures: Dict with 'pre' and 'post' arrays of shape (num_trials, num_nrns)\n",
    "        show_legend: Whether to show legend on this subplot\n",
    "    \"\"\"\n",
    "    num_nrns = measures[\"pre\"].shape[-1]\n",
    "    xticks = np.arange(1, num_nrns + 1)\n",
    "\n",
    "    # Plot pre (untrained) - dashed line\n",
    "    pre_mean = measures[\"pre\"].mean(axis=0)\n",
    "    ax.plot(xticks, pre_mean, ls=\":\", label=\"Untrained\", color=\"C0\")\n",
    "\n",
    "    # Plot post (trained) - solid line\n",
    "    post_mean = measures[\"post\"].mean(axis=0)\n",
    "    ax.plot(xticks, post_mean, ls=\"-\", label=\"Trained\", color=\"C1\")\n",
    "\n",
    "    # Styling to match reference notebook\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_xlim(1, num_nrns)\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    ax.grid(True)\n",
    "\n",
    "    if show_legend:\n",
    "        ax.legend(loc=\"lower left\", fontsize=\"x-small\")\n",
    "\n",
    "\n",
    "def create_side_grid_figure(\n",
    "    measures: dict[str, dict[str, dict[str, npt.NDArray[np.floating]]]],\n",
    "    sides: list[str],\n",
    "    dataset_name: str,\n",
    ") -> plt.Figure:\n",
    "    \"\"\"Create a grid figure with rows=sides, cols=conformations.\n",
    "\n",
    "    Args:\n",
    "        measures: Nested dict measures[side][conformation] -> {'pre': array, 'post': array}\n",
    "        sides: List of side names for row ordering\n",
    "        dataset_name: Dataset name for title\n",
    "\n",
    "    Returns:\n",
    "        Matplotlib figure\n",
    "    \"\"\"\n",
    "    n_rows = len(sides)\n",
    "    n_cols = len(CONFORMATIONS)\n",
    "\n",
    "    # Figure size: maintain aspect ratio similar to reference\n",
    "    fig_width = 5.5  # 3.5 * n_cols\n",
    "    fig_height = 4.0 / 3 * n_rows  # 2.0 * n_rows\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "        n_rows,\n",
    "        n_cols,\n",
    "        figsize=(fig_width, fig_height),\n",
    "        sharex=True,\n",
    "        sharey=True,\n",
    "    )\n",
    "\n",
    "    # Ensure axes is 2D\n",
    "    if n_rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "\n",
    "    for row, side in enumerate(sides):\n",
    "        for col, conformation in enumerate(CONFORMATIONS):\n",
    "            ax: plt.Axes = axes[row, col]\n",
    "\n",
    "            # Plot the rank curve\n",
    "            side_measures = measures[side][conformation]\n",
    "            # Show legend only on first subplot\n",
    "            show_legend = row == n_rows - 1 and col == 0\n",
    "            plot_rank_curve(ax, side_measures, show_legend=show_legend)\n",
    "\n",
    "            # Column titles (top row only)\n",
    "            if row == 0:\n",
    "                ax.set_title(dataset_name.upper() + \" - \" + conformation.capitalize(), fontweight=\"bold\")\n",
    "\n",
    "            # Row labels (left column only)\n",
    "            if col == 0:\n",
    "                ax.set_ylabel(r\"$\\mathcal{I}\\; (s, \\vec{R})$\")\n",
    "                ax.text(\n",
    "                    -0.4,\n",
    "                    0.5,\n",
    "                    rf\"{side.capitalize()}\",\n",
    "                    size=10,\n",
    "                    horizontalalignment=\"right\",\n",
    "                    verticalalignment=\"center\",\n",
    "                    transform=ax.transAxes,\n",
    "                    fontweight=\"bold\",\n",
    "                )\n",
    "\n",
    "            # X-axis label (bottom row only)\n",
    "            if row == n_rows - 1:\n",
    "                ax.set_xlabel(\"Neuron rank #\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Load Data\n",
    "\n",
    "Load precomputed measures for both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load N3P2 measures\n",
    "measures_n3p2 = load_dataset_measures(\"n3p2\", arch=ARCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load N4P2 measures\n",
    "measures_n4p2 = load_dataset_measures(\"n4p2\", arch=ARCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Generate Figures\n",
    "\n",
    "### N3P2: Side-Resolved Rank Curves (3×2 grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_n3p2 = create_side_grid_figure(measures_n3p2, SIDES_N3P2, \"n3p2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save N3P2 figure\n",
    "output_path_n3p2 = OUTPUT_DIR / \"fig_n3p2_side_rank_curves.pdf\"\n",
    "viz.save_figure(fig_n3p2, output_path_n3p2, overwrite=False, dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N4P2: Side-Resolved Rank Curves (4×2 grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_n4p2 = create_side_grid_figure(measures_n4p2, SIDES_N4P2, \"n4p2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save N4P2 figure\n",
    "output_path_n4p2 = OUTPUT_DIR / \"fig_n4p2_side_rank_curves.pdf\"\n",
    "viz.save_figure(fig_n4p2, output_path_n4p2, overwrite=False, dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Informative Neuron Statistics\n",
    "\n",
    "Compute the number and proportion of neurons exceeding the information threshold (2/3 bits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_informative_stats(\n",
    "    measures: dict[str, dict[str, dict[str, npt.NDArray[np.floating]]]],\n",
    "    sides: list[str],\n",
    "    dataset_name: str,\n",
    "    threshold: float = INFO_THRESHOLD,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Compute statistics for neurons exceeding the information threshold.\n",
    "\n",
    "    Args:\n",
    "        measures: Nested dict measures[side][conformation] -> {'pre': array, 'post': array}\n",
    "        sides: List of side names\n",
    "        dataset_name: Dataset identifier\n",
    "        threshold: Information threshold in bits (default: 2/3)\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with columns: dataset, side, conformation, condition,\n",
    "                                num_informative_mean, num_informative_sem,\n",
    "                                num_total, proportion_mean, proportion_sem\n",
    "    \"\"\"\n",
    "    records = []\n",
    "\n",
    "    for side in sides:\n",
    "        for conformation in CONFORMATIONS:\n",
    "            side_measures = measures[side][conformation]\n",
    "\n",
    "            for condition in [\"pre\", \"post\"]:\n",
    "                # Shape: (num_trials, num_nrns)\n",
    "                info_array = side_measures[condition]\n",
    "                num_trials, num_total = info_array.shape\n",
    "\n",
    "                # Count informative neurons per trial (per row)\n",
    "                informative_per_trial = (info_array >= threshold).sum(axis=1)\n",
    "\n",
    "                # Compute mean and SEM for counts\n",
    "                num_informative_mean = informative_per_trial.mean()\n",
    "                num_informative_sem = informative_per_trial.std(ddof=1) / np.sqrt(\n",
    "                    num_trials\n",
    "                )\n",
    "\n",
    "                # Compute proportion per trial and its statistics\n",
    "                proportion_per_trial = informative_per_trial / num_total\n",
    "                proportion_mean = proportion_per_trial.mean()\n",
    "                proportion_sem = proportion_per_trial.std(ddof=1) / np.sqrt(num_trials)\n",
    "\n",
    "                records.append(\n",
    "                    {\n",
    "                        \"dataset\": dataset_name,\n",
    "                        \"side\": side,\n",
    "                        \"conformation\": conformation,\n",
    "                        \"condition\": condition,\n",
    "                        \"num_informative_mean\": num_informative_mean,\n",
    "                        \"num_informative_sem\": num_informative_sem,\n",
    "                        \"num_total\": num_total,\n",
    "                        \"proportion_mean\": proportion_mean,\n",
    "                        \"proportion_sem\": proportion_sem,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    return pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute statistics for both datasets\n",
    "stats_n3p2 = compute_informative_stats(measures_n3p2, SIDES_N3P2, \"n3p2\")\n",
    "stats_n4p2 = compute_informative_stats(measures_n4p2, SIDES_N4P2, \"n4p2\")\n",
    "\n",
    "# Combine into single dataframe\n",
    "stats_df = pd.concat([stats_n3p2, stats_n4p2], ignore_index=True)\n",
    "\n",
    "# Display summary\n",
    "print(f\"Information threshold: {INFO_THRESHOLD:.4f} bits (2/3)\")\n",
    "print(f\"Total records: {len(stats_df)}\")\n",
    "stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df_pivot = stats_df.pivot_table(\n",
    "    index=[\"dataset\", \"side\", \"conformation\", \"condition\"],\n",
    "    values=[\n",
    "        \"num_informative_mean\",\n",
    "        \"num_informative_sem\",\n",
    "        \"proportion_mean\",\n",
    "        \"proportion_sem\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(5.5, 4.5), sharey=False)\n",
    "datasets = [\"n3p2\", \"n4p2\"]\n",
    "\n",
    "for row, dataset in enumerate(datasets):\n",
    "    for col, conf in enumerate(CONFORMATIONS):\n",
    "        ax = axes[row, col]\n",
    "        subset = stats_df[\n",
    "            (stats_df[\"dataset\"] == dataset) & (stats_df[\"conformation\"] == conf)\n",
    "        ]\n",
    "        sides = SIDES_N3P2 if dataset == \"n3p2\" else SIDES_N4P2\n",
    "\n",
    "        # Pivot for means and sems\n",
    "        mean_pivot = subset.pivot(\n",
    "            index=\"side\", columns=\"condition\", values=\"num_informative_mean\"\n",
    "        ).reindex(sides)\n",
    "        sem_pivot = subset.pivot(\n",
    "            index=\"side\", columns=\"condition\", values=\"num_informative_sem\"\n",
    "        ).reindex(sides)\n",
    "        prop_pivot = subset.pivot(\n",
    "            index=\"side\", columns=\"condition\", values=\"proportion_mean\"\n",
    "        ).reindex(sides)\n",
    "\n",
    "        # Bar positions\n",
    "        x = np.arange(len(sides))\n",
    "\n",
    "        # Width of a bar\n",
    "        width = 0.35\n",
    "        factor = 1 / 2 + 0.05\n",
    "\n",
    "        # Plot bars with error bars\n",
    "        bars_pre = ax.bar(\n",
    "            x - width * factor,\n",
    "            mean_pivot[\"pre\"],\n",
    "            width,\n",
    "            yerr=sem_pivot[\"pre\"],\n",
    "            label=\"Untrained\",\n",
    "            color=\"C0\",\n",
    "            capsize=3,\n",
    "        )\n",
    "        bars_post = ax.bar(\n",
    "            x + width * factor,\n",
    "            mean_pivot[\"post\"],\n",
    "            width,\n",
    "            yerr=sem_pivot[\"post\"],\n",
    "            label=\"Trained\",\n",
    "            color=\"C1\",\n",
    "            capsize=3,\n",
    "        )\n",
    "\n",
    "        # Add percentage labels above bars (accounting for error bars)\n",
    "        for bar, prop, sem in zip(bars_pre, prop_pivot[\"pre\"], sem_pivot[\"pre\"]):\n",
    "            pct = prop * 100\n",
    "            if pct > 0:\n",
    "                ax.annotate(\n",
    "                    f\"{pct:.1f}%\",\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, bar.get_height() + sem + 1),\n",
    "                    ha=\"center\",\n",
    "                    va=\"bottom\",\n",
    "                    fontsize=\"x-small\",\n",
    "                )\n",
    "        for bar, prop, sem in zip(bars_post, prop_pivot[\"post\"], sem_pivot[\"post\"]):\n",
    "            pct = prop * 100\n",
    "            if pct > 0:\n",
    "                ax.annotate(\n",
    "                    f\"{pct:.1f}%\",\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, bar.get_height() + sem + 1),\n",
    "                    ha=\"center\",\n",
    "                    va=\"bottom\",\n",
    "                    fontsize=\"x-small\",\n",
    "                )\n",
    "\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels([s.capitalize() for s in sides])\n",
    "        ax.set_ylim(0, 150)\n",
    "        ax.set_axisbelow(True)\n",
    "        ax.grid(axis=\"y\")\n",
    "\n",
    "        ax.set_title(f\"{dataset.upper()} - {conf.capitalize()}\", fontweight=\"bold\")\n",
    "        ax.set_ylabel(\"# informative neurons\" if col == 0 else \"\")\n",
    "        ax.set_xlabel(\"\")\n",
    "        if row == 1 and col == 0:\n",
    "            ax.legend(loc=\"upper right\", fontsize=\"small\")\n",
    "        ax.tick_params(axis=\"x\", rotation=0)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save informative neurons figure\n",
    "output_path_informative = OUTPUT_DIR / \"informative_neurons_sides_datasets.pdf\"\n",
    "viz.save_figure(fig, output_path_informative, overwrite=False, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "csv_path = OUTPUT_DIR / \"informative_neuron_stats.csv\"\n",
    "stats_df.to_csv(csv_path, index=False)\n",
    "print(f\"Saved: {csv_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hsnn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
