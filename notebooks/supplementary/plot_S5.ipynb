{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PNG detections: Retention across pipeline stages\n",
    "\n",
    "Stage-wise retention of three-neuron PNGs through the HFB detection pipeline.\n",
    "\n",
    "**Pipeline stages**\n",
    "\n",
    "1. Unconstrained (`hfb_unconstrained.db`): All detected triplet PNG candidates with layer structure `[L-1, L, L]` and synaptic connections\n",
    "\n",
    "2. Eligible (`hfb.db`): PNGs **after** applying manuscript eligibility criteria (i.e., delay-matched pathways to within 3 ms, synaptic weights w ≥ 0.5).\n",
    "\n",
    "3. Significant (`hfb_sgnf.db`): Final retained PNGs **after** statistical significance testing (surrogate-based).\n",
    "\n",
    "**Dependencies:**\n",
    "\n",
    "---\n",
    "\n",
    "A) Significance testing:\n",
    "- PNG detection and significance testing for N3P2: after network training\n",
    "- **This workflow is time-consuming to run**\n",
    "- Already existing detections will be skipped\n",
    "```bash\n",
    "./scripts/run_main_workflow.py experiments/n3p2/train_n3p2_lrate_0_04_181023 31 --rule significance --chkpt -1 -v\n",
    "```\n",
    "\n",
    "B) Unconstrained PNG detections:\n",
    "\n",
    "```bash\n",
    "./scripts/analysis/detection_unconstrained.py ./experiments/n3p2/train_n3p2_lrate_0_04_181023/ 31 --chkpt -1 -v\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Plots**\n",
    "\n",
    "- N3P2 and N4P2 Figs\n",
    "- Panel A: rank-order single neuron information curves\n",
    "- Panel B: number of selective neurons (exceeding 2/3 threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import hsnn.analysis.png.db as polydb\n",
    "from hsnn import viz, pipeline\n",
    "from hsnn.analysis.png import PNG\n",
    "from hsnn.simulation import Simulator\n",
    "from hsnn.utils import handler, io\n",
    "\n",
    "pidx = pd.IndexSlice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"n3p2/train_n3p2_lrate_0_04_181023\"\n",
    "CHECKPOINT_INDEX = -1\n",
    "\n",
    "# Analysis parameters\n",
    "LAYERS = [2, 3, 4]  # Layers to analyse (triplet structure [L-1, L, L])\n",
    "\n",
    "# DB retrieval parameters\n",
    "NRN_IDS = range(4096)\n",
    "POSITION = 1\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = io.BASE_DIR / \"out/figures/supplementary/fig_S5\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Setup plotting\n",
    "viz.setup_journal_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Experiment and Identify Detection Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expt = handler.ExperimentHandler(EXPERIMENT_NAME)\n",
    "print(f\"Experiment: {expt.logdir.relative_to(io.BASE_DIR)}\")\n",
    "\n",
    "# Get trial view(s)\n",
    "trials = [expt[\"TrainSNN_eb0d4_00031\"]]\n",
    "print(f\"Selected trial(s): {trials}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Database Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_db(\n",
    "    trial: handler.TrialView, db_name: str, chkpt_idx: int = -1\n",
    ") -> polydb.PNGDatabase | None:\n",
    "    \"\"\"Load a PNG database by name for a given trial.\n",
    "\n",
    "    Args:\n",
    "        trial: Trial view object\n",
    "        db_name: One of 'hfb_unconstrained.db', 'hfb.db', 'hfb_sgnf.db'\n",
    "        chkpt_idx: Checkpoint index (-1 for last)\n",
    "\n",
    "    Returns:\n",
    "        PNGDatabase or None if not found\n",
    "    \"\"\"\n",
    "    checkpoint = trial.checkpoints[chkpt_idx]\n",
    "    db_path = checkpoint.path / db_name\n",
    "\n",
    "    if not db_path.exists():\n",
    "        return None\n",
    "\n",
    "    return polydb.PNGDatabase(db_path)\n",
    "\n",
    "\n",
    "def load_syn_params(trial: handler.TrialView, chkpt_idx: int = -1) -> pd.DataFrame:\n",
    "    \"\"\"Load synaptic parameters for a trial checkpoint.\n",
    "\n",
    "    Args:\n",
    "        trial: Trial view object\n",
    "        chkpt_idx: Checkpoint index (-1 for last)\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with synaptic parameters indexed by [layer, proj, pre, post]\n",
    "    \"\"\"\n",
    "    checkpoint = trial.checkpoints[chkpt_idx]\n",
    "    sim = Simulator.from_config_restore(trial.config, file_path=checkpoint.store_path)\n",
    "    return sim.network.get_syn_params(return_delays=True)\n",
    "\n",
    "\n",
    "def get_png_set(\n",
    "    db: polydb.PNGDatabase, layer: int, nrn_ids: range, position: int\n",
    ") -> set[PNG]:\n",
    "    \"\"\"Extract unique PNGs for a layer as a set.\n",
    "\n",
    "    Uses PNG __hash__ which is based on (layers, nrns) tuples only.\n",
    "    \"\"\"\n",
    "    polygrps = db.get_pngs(layer, nrn_ids, position)\n",
    "    return set(polygrps)\n",
    "\n",
    "\n",
    "def get_filtered_png_set(\n",
    "    db: polydb.PNGDatabase,\n",
    "    layer: int,\n",
    "    nrn_ids: range,\n",
    "    position: int,\n",
    "    syn_params: pd.DataFrame,\n",
    ") -> tuple[set[PNG], dict]:\n",
    "    \"\"\"Extract unique PNGs for a layer, filtered for valid HFB connectivity.\n",
    "\n",
    "    Applies filter_valid_hfb_triplets to handle tied-lag ambiguity and\n",
    "    verify synaptic connectivity.\n",
    "\n",
    "    Args:\n",
    "        db: PNG database\n",
    "        layer: Layer to query\n",
    "        nrn_ids: Neuron IDs to include\n",
    "        position: Position index for query\n",
    "        syn_params: Synaptic parameters for connectivity checking\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (png_set, filter_stats)\n",
    "    \"\"\"\n",
    "    polygrps = db.get_pngs(layer, nrn_ids, position)\n",
    "    valid_pngs, stats = pipeline.filter_valid_hfb_triplets(polygrps, syn_params)\n",
    "    return set(valid_pngs), stats\n",
    "\n",
    "\n",
    "def validate_layer_structure(png: PNG, expected_layer: int) -> bool:\n",
    "    \"\"\"Validate that PNG has expected [L-1, L, L] structure.\"\"\"\n",
    "    expected_structure = [expected_layer - 1, expected_layer, expected_layer]\n",
    "    return list(png.layers) == expected_structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Databases for All Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_NAMES = {\n",
    "    \"unconstrained\": \"hfb_unconstrained.db\",\n",
    "    \"eligible\": \"hfb.db\",\n",
    "    \"significant\": \"hfb_sgnf.db\",\n",
    "}\n",
    "\n",
    "# Load databases for all trials\n",
    "trial_dbs: dict[str, dict[str, polydb.PNGDatabase]] = {}\n",
    "\n",
    "for trial in tqdm(trials, desc=\"Loading trial databases\"):\n",
    "    trial_id = trial.name\n",
    "    dbs = {}\n",
    "\n",
    "    for stage, db_name in DB_NAMES.items():\n",
    "        db = load_db(trial, db_name, CHECKPOINT_INDEX)\n",
    "        if db is not None:\n",
    "            dbs[stage] = db\n",
    "            print(f\"Trial {trial_id}: Loaded {stage} ({db_name})\")\n",
    "        else:\n",
    "            print(f\"Trial {trial_id}: Missing {stage} ({db_name})\")\n",
    "\n",
    "    if len(dbs) == 3:  # Only include trials with all three DBs\n",
    "        trial_dbs[trial_id] = dbs\n",
    "\n",
    "print(f\"\\nTrials with complete DB sets: {len(trial_dbs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extract PNG Sets and Compute Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_layer_stats(\n",
    "    dbs: dict,\n",
    "    layer: int,\n",
    "    nrn_ids: range,\n",
    "    position: int,\n",
    "    syn_params: pd.DataFrame,\n",
    ") -> dict:\n",
    "    \"\"\"Compute PNG counts and fractions for a single layer.\n",
    "\n",
    "    Applies connectivity filtering to unconstrained PNGs to ensure fair\n",
    "    comparison across pipeline stages.\n",
    "\n",
    "    Args:\n",
    "        dbs: Dict mapping stage names to PNGDatabase objects\n",
    "        layer: Layer to analyze (triplet structure [L-1, L, L])\n",
    "        nrn_ids: Range of neuron IDs to include\n",
    "        position: Position index for DB query\n",
    "        syn_params: Synaptic parameters for connectivity filtering\n",
    "\n",
    "    Returns:\n",
    "        Dict with counts, fractions, and PNG sets\n",
    "    \"\"\"\n",
    "    # Extract PNG sets - filter unconstrained for valid connectivity\n",
    "    U, filter_stats = get_filtered_png_set(\n",
    "        dbs[\"unconstrained\"], layer, nrn_ids, position, syn_params\n",
    "    )\n",
    "    E = get_png_set(dbs[\"eligible\"], layer, nrn_ids, position)\n",
    "    S = get_png_set(dbs[\"significant\"], layer, nrn_ids, position)\n",
    "\n",
    "    # Validate layer structure\n",
    "    for png_set, stage_name in [\n",
    "        (U, \"unconstrained\"),\n",
    "        (E, \"eligible\"),\n",
    "        (S, \"significant\"),\n",
    "    ]:\n",
    "        for png in png_set:\n",
    "            assert validate_layer_structure(png, layer), (\n",
    "                f\"Invalid layer structure in {stage_name}: {png.layers} \"\n",
    "                f\"(expected [{layer - 1}, {layer}, {layer}])\"\n",
    "            )\n",
    "\n",
    "    # Compute counts\n",
    "    n_unconstrained = len(U)\n",
    "    n_eligible = len(E)\n",
    "    n_significant = len(S)\n",
    "\n",
    "    # Sanity checks\n",
    "    assert n_eligible <= n_unconstrained, (\n",
    "        f\"Layer {layer}: N_eligible ({n_eligible}) > N_unconstrained ({n_unconstrained})\"\n",
    "    )\n",
    "    assert n_significant <= n_eligible, (\n",
    "        f\"Layer {layer}: N_significant ({n_significant}) > N_eligible ({n_eligible})\"\n",
    "    )\n",
    "\n",
    "    # Compute fractions (handle empty denominators)\n",
    "    frac_eligible = n_eligible / n_unconstrained if n_unconstrained > 0 else np.nan\n",
    "    frac_significant = n_significant / n_eligible if n_eligible > 0 else np.nan\n",
    "\n",
    "    return {\n",
    "        \"n_unconstrained\": n_unconstrained,\n",
    "        \"n_eligible\": n_eligible,\n",
    "        \"n_significant\": n_significant,\n",
    "        \"frac_eligible\": frac_eligible,\n",
    "        \"frac_significant_given_eligible\": frac_significant,\n",
    "        \"filter_stats\": filter_stats,  # Stats from connectivity filtering\n",
    "        \"U\": U,\n",
    "        \"E\": E,\n",
    "        \"S\": S,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes ~2.5 min to run (AMD Ryzen 9 5900X, 64GB RAM)\n",
    "# Cache path for computed statistics\n",
    "cache_path = OUTPUT_DIR / \".cache/spade_proportions_stats.pkl\"\n",
    "\n",
    "if cache_path.exists():\n",
    "    print(f\"Loading cached results from: {cache_path}\")\n",
    "    cached = io.load_pickle(cache_path)\n",
    "    results_df = cached[\"results_df\"]\n",
    "    pooled_sets = cached[\"pooled_sets\"]\n",
    "    filter_stats_all = cached.get(\"filter_stats_all\", [])\n",
    "    print(f\"Loaded statistics for {len(results_df)} (trial, layer) combinations\")\n",
    "else:\n",
    "    # Compute statistics for each trial and layer\n",
    "    all_results = []\n",
    "    filter_stats_all = []\n",
    "\n",
    "    # Also accumulate PNG sets for pooled analysis\n",
    "    pooled_sets = {\"U\": set(), \"E\": set(), \"S\": set()}\n",
    "\n",
    "    for trial_id, dbs in tqdm(trial_dbs.items(), desc=\"Computing statistics\"):\n",
    "        # Load synaptic parameters for this trial\n",
    "        trial = [t for t in trials if t.name == trial_id][0]\n",
    "        syn_params = load_syn_params(trial, CHECKPOINT_INDEX)\n",
    "        print(f\"Trial {trial_id}: Loaded syn_params\")\n",
    "\n",
    "        for layer in LAYERS:\n",
    "            try:\n",
    "                stats = compute_layer_stats(dbs, layer, NRN_IDS, POSITION, syn_params)\n",
    "\n",
    "                all_results.append(\n",
    "                    {\n",
    "                        \"trial_id\": trial_id,\n",
    "                        \"layer\": layer,\n",
    "                        \"n_unconstrained\": stats[\"n_unconstrained\"],\n",
    "                        \"n_eligible\": stats[\"n_eligible\"],\n",
    "                        \"n_significant\": stats[\"n_significant\"],\n",
    "                        \"frac_eligible\": stats[\"frac_eligible\"],\n",
    "                        \"frac_significant_given_eligible\": stats[\n",
    "                            \"frac_significant_given_eligible\"\n",
    "                        ],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                # Store filtering stats\n",
    "                filter_stats_all.append(\n",
    "                    {\"trial_id\": trial_id, \"layer\": layer, **stats[\"filter_stats\"]}\n",
    "                )\n",
    "\n",
    "                # Accumulate for pooled analysis\n",
    "                pooled_sets[\"U\"].update(stats[\"U\"])\n",
    "                pooled_sets[\"E\"].update(stats[\"E\"])\n",
    "                pooled_sets[\"S\"].update(stats[\"S\"])\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Trial {trial_id}, Layer {layer}: Error - {e}\")\n",
    "                raise\n",
    "\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    print(f\"\\nComputed statistics for {len(results_df)} (trial, layer) combinations\")\n",
    "\n",
    "    # Save to cache\n",
    "    io.save_pickle(\n",
    "        {\n",
    "            \"results_df\": results_df,\n",
    "            \"pooled_sets\": pooled_sets,\n",
    "            \"filter_stats_all\": filter_stats_all,\n",
    "        },\n",
    "        cache_path,\n",
    "        parents=True,\n",
    "    )\n",
    "    print(f\"Saved results to cache: {cache_path}\")\n",
    "\n",
    "results_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display filtering statistics (connectivity validation of unconstrained PNGs)\n",
    "if filter_stats_all:\n",
    "    filter_df = pd.DataFrame(filter_stats_all)\n",
    "    print(\"Connectivity filtering of unconstrained PNGs:\")\n",
    "    print(filter_df.to_string(index=False))\n",
    "    print()\n",
    "\n",
    "    # Aggregate totals\n",
    "    totals = filter_df[[\"n_total\", \"n_valid\", \"n_tied_lags\", \"n_tied_lag_duplicates\", \"n_no_connectivity\"]].sum()\n",
    "    print(\"Aggregated filtering totals:\")\n",
    "    print(f\"  Raw detections:        {totals['n_total']:,}\")\n",
    "    print(f\"  Valid (after filter):  {totals['n_valid']:,}\")\n",
    "    print(f\"  - Tied H/B lags:       {totals['n_tied_lags']:,}\")\n",
    "    print(f\"  - Tied-lag duplicates: {totals['n_tied_lag_duplicates']:,}\")\n",
    "    print(f\"  - No connectivity:     {totals['n_no_connectivity']:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Aggregate Statistics by Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate by layer: sum counts across trials\n",
    "layer_agg = (\n",
    "    results_df.groupby(\"layer\")\n",
    "    .agg({\"n_unconstrained\": \"sum\", \"n_eligible\": \"sum\", \"n_significant\": \"sum\"})\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Compute fractions from aggregated counts\n",
    "layer_agg[\"frac_eligible\"] = layer_agg[\"n_eligible\"] / layer_agg[\"n_unconstrained\"]\n",
    "layer_agg[\"frac_significant_given_eligible\"] = (\n",
    "    layer_agg[\"n_significant\"] / layer_agg[\"n_eligible\"]\n",
    ")\n",
    "\n",
    "layer_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute pooled statistics using set union across all trials/layers\n",
    "n_unconstrained_pooled = len(pooled_sets[\"U\"])\n",
    "n_eligible_pooled = len(pooled_sets[\"E\"])\n",
    "n_significant_pooled = len(pooled_sets[\"S\"])\n",
    "\n",
    "frac_eligible_pooled = (\n",
    "    n_eligible_pooled / n_unconstrained_pooled if n_unconstrained_pooled > 0 else np.nan\n",
    ")\n",
    "frac_significant_pooled = (\n",
    "    n_significant_pooled / n_eligible_pooled if n_eligible_pooled > 0 else np.nan\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Pooled (L2-4): U={n_unconstrained_pooled}, E={n_eligible_pooled}, S={n_significant_pooled}\"\n",
    ")\n",
    "print(\n",
    "    f\"frac_eligible={frac_eligible_pooled:.4f}, frac_significant={frac_significant_pooled:.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Build Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build final summary table\n",
    "summary_rows = []\n",
    "\n",
    "for _, row in layer_agg.iterrows():\n",
    "    summary_rows.append(\n",
    "        {\n",
    "            \"layer\": f\"L{int(row['layer'])}\",\n",
    "            \"N_unconstrained\": int(row[\"n_unconstrained\"]),\n",
    "            \"N_eligible\": int(row[\"n_eligible\"]),\n",
    "            \"N_significant\": int(row[\"n_significant\"]),\n",
    "            \"frac_eligible\": row[\"frac_eligible\"],\n",
    "            \"frac_significant_given_eligible\": row[\"frac_significant_given_eligible\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Add pooled row\n",
    "summary_rows.append(\n",
    "    {\n",
    "        \"layer\": \"All (2-4)\",\n",
    "        \"N_unconstrained\": n_unconstrained_pooled,\n",
    "        \"N_eligible\": n_eligible_pooled,\n",
    "        \"N_significant\": n_significant_pooled,\n",
    "        \"frac_eligible\": frac_eligible_pooled,\n",
    "        \"frac_significant_given_eligible\": frac_significant_pooled,\n",
    "    }\n",
    ")\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_df = summary_df.set_index(\"layer\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PNG RETENTION BY PIPELINE STAGE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Experiment: {EXPERIMENT_NAME}\")\n",
    "print(f\"Trials analysed: {len(trial_dbs)}\")\n",
    "print(f\"Layers: {LAYERS}\")\n",
    "print(\"=\" * 80)\n",
    "summary_df.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Summary Table as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = OUTPUT_DIR / \"hfb_retention_fractions_by_stage.csv\"\n",
    "summary_df.to_csv(csv_path)\n",
    "print(f\"Saved summary table: {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Generate Figure\n",
    "\n",
    "A single-panel grouped bar chart, showing PNG counts at each pipeline stage (Detected, Eligible, Significant) broken down by layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for plotting\n",
    "layer_labels = list(summary_df.index)\n",
    "frac_eligible = summary_df[\"frac_eligible\"].values\n",
    "frac_significant = summary_df[\"frac_significant_given_eligible\"].values\n",
    "\n",
    "# Get numerators/denominators for annotations\n",
    "n_u = summary_df[\"N_unconstrained\"].values\n",
    "n_e = summary_df[\"N_eligible\"].values\n",
    "n_s = summary_df[\"N_significant\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare per-layer data (excluding the pooled \"All\" row)\n",
    "layer_data = summary_df.iloc[:-1].copy()  # L2, L3, L4 only\n",
    "pooled_data = summary_df.iloc[-1]  # \"All (2-4)\" row\n",
    "\n",
    "# Extract layer indices and counts\n",
    "layers = [int(l[1]) for l in layer_data.index]  # [2, 3, 4]\n",
    "layer_labels_plot = [f\"L{l}\" for l in layers]\n",
    "\n",
    "# Counts per stage per layer\n",
    "counts_U = layer_data[\"N_unconstrained\"].astype(int).values\n",
    "counts_E = layer_data[\"N_eligible\"].astype(int).values\n",
    "counts_S = layer_data[\"N_significant\"].astype(int).values\n",
    "\n",
    "# Pooled totals\n",
    "total_U = int(pooled_data[\"N_unconstrained\"])\n",
    "total_E = int(pooled_data[\"N_eligible\"])\n",
    "total_S = int(pooled_data[\"N_significant\"])\n",
    "\n",
    "# Retention percentages\n",
    "pct_e_of_u = total_E / total_U * 100\n",
    "pct_s_of_e = total_S / total_E * 100\n",
    "pct_s_of_u = total_S / total_U * 100\n",
    "\n",
    "print(f\"Total counts: U={total_U:,}, E={total_E:,}, S={total_S:,}\")\n",
    "print(\n",
    "    f\"Retention: E/U = {pct_e_of_u:.1f}%, S/E = {pct_s_of_e:.1f}%, S/U = {pct_s_of_u:.1f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grouped bar chart with legend/stats box on the right\n",
    "fig, ax = plt.subplots(figsize=(5.5, 2))\n",
    "\n",
    "# Add a small gap between layer groups\n",
    "x = np.arange(len(layer_labels_plot)) * 1.2  # L2, L3, L4 only\n",
    "width = 0.28\n",
    "bar_offset = width * 1.1  # adds a small gap between adjacent bars within each group\n",
    "\n",
    "# Create bars with standard matplotlib colors (per-layer only, no Total)\n",
    "bars_U = ax.bar(\n",
    "    x - bar_offset,\n",
    "    counts_U,\n",
    "    width,\n",
    "    label=\"Detected (D)\",\n",
    "    color=\"C0\",\n",
    "    edgecolor=\"black\",\n",
    "    linewidth=0.5,\n",
    ")\n",
    "bars_E = ax.bar(\n",
    "    x,\n",
    "    counts_E,\n",
    "    width,\n",
    "    label=\"Eligible (E)\",\n",
    "    color=\"C1\",\n",
    "    edgecolor=\"black\",\n",
    "    linewidth=0.5,\n",
    ")\n",
    "bars_S = ax.bar(\n",
    "    x + bar_offset,\n",
    "    counts_S,\n",
    "    width,\n",
    "    label=\"Significant (S)\",\n",
    "    color=\"C2\",\n",
    "    edgecolor=\"black\",\n",
    "    linewidth=0.5,\n",
    ")\n",
    "\n",
    "# Scientific notation formatter for y-axis\n",
    "\n",
    "ax.yaxis.set_major_formatter(ScalarFormatter(useMathText=True))\n",
    "ax.ticklabel_format(style=\"sci\", axis=\"y\", scilimits=(0, 0))\n",
    "\n",
    "# Formatting\n",
    "ax.set_xlabel(\"Layer\")\n",
    "ax.set_ylabel(\"# PNGs\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(layer_labels_plot)\n",
    "ax.set_axisbelow(True)\n",
    "ax.set_ylim(0, 5e4)\n",
    "ax.grid(axis=\"y\", alpha=1)\n",
    "\n",
    "# Make room on the right for legend and stats box\n",
    "fig.subplots_adjust(right=0.68)\n",
    "\n",
    "# Place legend outside axes, aligned to top-right\n",
    "legend = ax.legend(\n",
    "    loc=\"upper left\",\n",
    "    bbox_to_anchor=(1.02, 1.0),\n",
    "    bbox_transform=ax.transAxes,\n",
    "    borderaxespad=0.0,\n",
    "    fontsize=\"small\",\n",
    "    frameon=True,\n",
    "    edgecolor=\"gray\",\n",
    ")\n",
    "\n",
    "# Add retention percentage annotation box below legend, outside axes\n",
    "textstr = (\n",
    "    f\"Overall (n = {total_U:,}):\\n\"\n",
    "    f\"  E/D = {pct_e_of_u:.1f}%\\n\"\n",
    "    f\"  S/E = {pct_s_of_e:.1f}%\\n\"\n",
    "    f\"  S/D = {pct_s_of_u:.1f}%\"\n",
    ")\n",
    "props = dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", edgecolor=\"gray\")\n",
    "ax.text(\n",
    "    1.03,\n",
    "    0.55,\n",
    "    textstr,\n",
    "    transform=ax.transAxes,\n",
    "    fontsize=8,\n",
    "    verticalalignment=\"top\",\n",
    "    bbox=props,\n",
    "    # family=\"monospace\",\n",
    ")\n",
    "\n",
    "# Save figure\n",
    "pdf_path_supp = OUTPUT_DIR / \"fig_hfb_proportions.pdf\"\n",
    "viz.save_figure(fig, pdf_path_supp, overwrite=False, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"SUMMARY: PNG Retention Across Pipeline Stages\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nExperiment: {EXPERIMENT_NAME}\")\n",
    "print(f\"Trials analysed: {len(trial_dbs)}\")\n",
    "print(f\"Layers: {LAYERS}\")\n",
    "print()\n",
    "print(\"Database stages:\")\n",
    "print(\"  U = Unconstrained (hfb_unconstrained.db) - all detected triplets\")\n",
    "print(\"  E = Eligible (hfb.db) - after HFB criteria (w>=0.5, δt=±3ms)\")\n",
    "print(\"  S = Significant (hfb_sgnf.db) - after surrogate-based testing\")\n",
    "print()\n",
    "print(\"-\" * 70)\n",
    "print(\"Results by Layer:\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for idx in summary_df.index:\n",
    "    row = summary_df.loc[idx]\n",
    "    print(f\"\\n{idx}:\")\n",
    "    print(f\"  Unconstrained (U): {int(row['N_unconstrained']):,}\")\n",
    "    print(\n",
    "        f\"  Eligible (E):      {int(row['N_eligible']):,} ({row['frac_eligible']:.1%} of U)\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  Significant (S):   {int(row['N_significant']):,} ({row['frac_significant_given_eligible']:.1%} of E)\"\n",
    "    )\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Interpretation:\")\n",
    "print(\"- Eligibility filtering retains PNGs with strengthened synapses (w>=0.5)\")\n",
    "print(\"  and timing consistent with axonal delays (δt=±3ms)\")\n",
    "print(\"- Significance testing removes PNGs that could arise by chance\")\n",
    "print(\"  in shuffled surrogate data\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close database connections\n",
    "for trial_id, dbs in trial_dbs.items():\n",
    "    for stage, db in dbs.items():\n",
    "        db.close()\n",
    "print(\"Closed all database connections\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hsnn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
