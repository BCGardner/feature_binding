{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robustness & Sensitivity Analysis for HFB Detection\n",
    "\n",
    "This notebook explores:\n",
    "- Sensitivity of detected PNG counts to temporal-span and timing-tolerance parameters.\n",
    "- Compositional reuse of high-level feature neurons across multiple PNGs.\n",
    "\n",
    "**Dependencies:**\n",
    "\n",
    "---\n",
    "\n",
    "Significance testing:\n",
    "- PNG detection and significance testing for N3P2: after network training\n",
    "- **This workflow is time-consuming to run**\n",
    "- Already existing detections will be skipped\n",
    "```bash\n",
    "./scripts/run_main_workflow.py experiments/n3p2/train_n3p2_lrate_0_04_181023 31 --rule significance --chkpt -1 -v\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Analyses performed**\n",
    "\n",
    "1. Window Sensitivity:\n",
    "    - Filter significant HFBs by empirical temporal span and report counts per layer.\n",
    "2. Timing Tolerance Strictness:\n",
    "    - Re-apply delay-matching criterion with tighter tolerances (δt = 0, 1, 2, 3 ms) to assess robustness of HFB classification.\n",
    "3. Composition Metric:\n",
    "    - Count distinct HFB circuits per high-level neuron to provide evidence that triplets form larger assemblies.\n",
    "\n",
    "**Plots:**\n",
    "\n",
    "- Supplementary S7 Fig\n",
    "- Supplementary S8 Fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import Sequence\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import hsnn.analysis.png.db as polydb\n",
    "from hsnn import viz\n",
    "from hsnn.analysis.png import PNG, refinery\n",
    "from hsnn.simulation import Simulator\n",
    "from hsnn.utils import handler, io\n",
    "\n",
    "from hsnn.analysis.png.refinery import FilterBySpan\n",
    "\n",
    "pidx = pd.IndexSlice\n",
    "\n",
    "# Setup plotting\n",
    "viz.setup_journal_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURATION ===\n",
    "\n",
    "# Experiment settings\n",
    "EXPERIMENT_NAME = \"n3p2/train_n3p2_lrate_0_04_181023\"\n",
    "TRIAL_INDEX = 31\n",
    "CHECKPOINT_INDEX = -1\n",
    "\n",
    "# Analysis settings\n",
    "RESTRICT_LAYERS = None  # Set to [4] for quick L4-only analysis, None for all layers\n",
    "SPAN_THRESHOLDS_MS = [4, 8, 12, 16, 20]  # Window sensitivity thresholds\n",
    "DT_THRESHOLDS_MS = [0, 1, 2, 3]  # Timing tolerance strictness (baseline is 3 ms)\n",
    "\n",
    "# HFB constraint parameters (baseline)\n",
    "W_MIN = 0.5  # Minimum synaptic weight\n",
    "TOL_BASELINE = 3.0  # Baseline delay tolerance (ms)\n",
    "\n",
    "# PNG database settings\n",
    "POSITION = 1  # Second-firing neuron (high-level) index position\n",
    "NRN_IDS = range(4096)  # All possible neuron IDs\n",
    "\n",
    "# Setup output directory\n",
    "OUTPUT_DIR = io.BASE_DIR / \"out/figures/supplementary/fig_S7_S8\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Results will be saved to: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Experiment Data\n",
    "\n",
    "Load the trained network's synaptic parameters and significant HFB detections from the canonical database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load experiment and trial\n",
    "expt = handler.ExperimentHandler(EXPERIMENT_NAME)\n",
    "trial = expt[TRIAL_INDEX]\n",
    "\n",
    "print(f\"Experiment: {expt.name}\")\n",
    "print(f\"Trial: {trial.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load network and synaptic parameters\n",
    "store = handler.ArtifactStore(trial, ckpt_idx=CHECKPOINT_INDEX)\n",
    "cfg = trial.config\n",
    "\n",
    "sim = Simulator.from_config(cfg)\n",
    "sim.restore(store.checkpoint.store_path)\n",
    "print(f\"Restored network from: {store.checkpoint.store_path.relative_to(expt.logdir.parent)}\")\n",
    "\n",
    "# Get synaptic parameters (weights and delays)\n",
    "syn_params: pd.DataFrame = sim.network.get_syn_params(return_delays=True)\n",
    "\n",
    "# Filter to plastic projections (FF, E2E)\n",
    "projs_plastic = ('FF', 'E2E')\n",
    "mask = syn_params.index.get_level_values('proj').isin(projs_plastic)\n",
    "syn_params = syn_params.loc[mask].copy()\n",
    "\n",
    "print(f\"\\nSynaptic parameters shape: {syn_params.shape}\")\n",
    "print(f\"Projections: {syn_params.index.get_level_values('proj').unique().tolist()}\")\n",
    "print(f\"Layers: {syn_params.index.get_level_values('layer').unique().tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load significant HFB detections\n",
    "db_sgnf = polydb.PNGDatabase.from_trial(trial, CHECKPOINT_INDEX, sgnf=True)\n",
    "\n",
    "if not db_sgnf.exists:\n",
    "    raise FileNotFoundError(f\"Significant HFB database not found: {db_sgnf.path}\")\n",
    "\n",
    "print(f\"Loaded significant HFB database: {db_sgnf.path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine layers to analyse\n",
    "available_layers = sorted(syn_params.index.get_level_values('layer').unique())\n",
    "if RESTRICT_LAYERS is not None:\n",
    "    layers_to_analyse = [layer for layer in RESTRICT_LAYERS if layer in available_layers]\n",
    "else:\n",
    "    layers_to_analyse = available_layers\n",
    "\n",
    "print(f\"Layers to analyze: {layers_to_analyse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all significant HFBs for the layers of interest\n",
    "polygrps_by_layer: dict[int, list[PNG]] = {}\n",
    "\n",
    "for layer in layers_to_analyse:\n",
    "    polygrps = db_sgnf.get_pngs(layer, NRN_IDS, POSITION)\n",
    "    polygrps_by_layer[layer] = polygrps\n",
    "    print(f\"Layer {layer}: {len(polygrps)} significant HFBs\")\n",
    "\n",
    "total_hfbs = sum(len(p) for p in polygrps_by_layer.values())\n",
    "print(f\"\\nTotal significant HFBs: {total_hfbs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Window Sensitivity Analysis\n",
    "\n",
    "For each significant HFB circuit, compute the empirical temporal span:\n",
    "\n",
    "$$\\text{span}_{\\text{ms}} = t_{\\text{last}} - t_{\\text{first}} = \\max(\\text{lags}) - \\min(\\text{lags})$$\n",
    "\n",
    "Then filter by effective window thresholds and report counts per layer.\n",
    "\n",
    "This analysis tests whether HFB detection results are robust to the choice of temporal window. If counts remain stable across different span thresholds, it suggests the detected circuits are not artifacts of the specific window choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_png_spans(pngs: Sequence[PNG]) -> np.ndarray:\n",
    "    \"\"\"Extract temporal spans from a sequence of PNGs.\"\"\"\n",
    "    return np.array([float(np.max(p.lags) - np.min(p.lags)) for p in pngs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute span distribution for all HFBs\n",
    "all_spans = []\n",
    "for layer, polygrps in polygrps_by_layer.items():\n",
    "    spans = get_png_spans(polygrps)\n",
    "    all_spans.extend(spans)\n",
    "    print(f\"Layer {layer}: span range [{spans.min():.1f}, {spans.max():.1f}] ms, \"\n",
    "          f\"mean={spans.mean():.2f} ms, median={np.median(spans):.2f} ms\")\n",
    "\n",
    "all_spans = np.array(all_spans)\n",
    "print(f\"\\nOverall: span range [{all_spans.min():.1f}, {all_spans.max():.1f}] ms, \"\n",
    "      f\"mean={all_spans.mean():.2f} ms, median={np.median(all_spans):.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot span distribution\n",
    "fig, ax = plt.subplots(figsize=(5.5, 2))\n",
    "\n",
    "bins = np.arange(0, max(all_spans) + 2, 1)\n",
    "ax.hist(all_spans, bins=bins, edgecolor='black', alpha=1)\n",
    "ax.axvline(all_spans.mean(), color='C1', linestyle='--', label=f'Mean ({all_spans.mean():.1f} ms)')\n",
    "\n",
    "ax.set_xticks(np.arange(0, 22, 2))\n",
    "ax.set_xlim(0, 20)\n",
    "ax.set_xlabel('Temporal span [ms]')\n",
    "ax.set_ylabel('# PNGs')\n",
    "ax.legend()\n",
    "ax.set_axisbelow(True)\n",
    "ax.grid(True)\n",
    "\n",
    "fig.tight_layout()\n",
    "viz.save_figure(fig, OUTPUT_DIR / 'span_distribution.pdf', overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window sensitivity: count HFBs per layer per span threshold\n",
    "span_sensitivity_results = []\n",
    "\n",
    "for span_thresh in SPAN_THRESHOLDS_MS:\n",
    "    filter_span = FilterBySpan(max_span=span_thresh)\n",
    "\n",
    "    for layer, polygrps in polygrps_by_layer.items():\n",
    "        filtered = filter_span(polygrps)\n",
    "        span_sensitivity_results.append({\n",
    "            'span_threshold_ms': span_thresh,\n",
    "            'layer': layer,\n",
    "            'count': len(filtered),\n",
    "            'count_original': len(polygrps),\n",
    "            'retention_pct': 100 * len(filtered) / len(polygrps) if len(polygrps) > 0 else 0\n",
    "        })\n",
    "\n",
    "span_df = pd.DataFrame(span_sensitivity_results)\n",
    "print(\"Window Sensitivity Results:\")\n",
    "print(span_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot table for cleaner display\n",
    "span_pivot = span_df.pivot(index='layer', columns='span_threshold_ms', values='count')\n",
    "span_pivot.columns = [f'≤{c} ms' for c in span_pivot.columns]\n",
    "span_pivot.index = [f'L{i}' for i in span_pivot.index]\n",
    "\n",
    "# Add totals\n",
    "span_pivot.loc['Total'] = span_pivot.sum()\n",
    "\n",
    "print(\"\\nHFB Counts by Span Threshold:\")\n",
    "print(span_pivot)\n",
    "\n",
    "# Save to CSV\n",
    "span_pivot.to_csv(OUTPUT_DIR / 'sensitivity_counts_by_span.csv')\n",
    "print(f\"\\nSaved: {OUTPUT_DIR / 'sensitivity_counts_by_span.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot: HFB counts vs span threshold\n",
    "fig, ax = plt.subplots(figsize=(5.5, 2.5))\n",
    "\n",
    "for idx, layer in enumerate(layers_to_analyse):\n",
    "    layer_data = span_df[span_df['layer'] == layer]\n",
    "    ax.plot(layer_data['span_threshold_ms'], layer_data['count'],\n",
    "            marker='o', label=f'L{layer}', linewidth=1.5, markersize=6, alpha=1)\n",
    "\n",
    "# Add total line\n",
    "total_counts = span_df.groupby('span_threshold_ms')['count'].sum()\n",
    "ax.plot(total_counts.index, total_counts.values,\n",
    "        marker='s', label='Total', linewidth=1.5, markersize=6,\n",
    "        color='black', linestyle='--')\n",
    "\n",
    "ax.set_xlabel('Maximum span threshold [ms]')\n",
    "ax.set_ylabel('# PNGs')\n",
    "ax.legend(loc='best', fontsize=\"small\")\n",
    "ax.grid(True, alpha=1)\n",
    "ax.set_xticks(SPAN_THRESHOLDS_MS)\n",
    "ax.set_ylim(None, 10000 * 1.11)\n",
    "\n",
    "fig.tight_layout()\n",
    "viz.save_figure(fig, OUTPUT_DIR / 'sensitivity_window.pdf', overwrite=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Timing Tolerance (δt) Strictness Analysis\n",
    "\n",
    "Re-apply the delay-matching criterion with **stricter** tolerances to assess robustness of HFB classification.\n",
    "\n",
    "**HFB delay constraint**\n",
    "\n",
    "For a 3-neuron HFB circuit with neurons (L, H, B) in firing order:\n",
    "- The composite delay relation must hold: $d_{L \\to B} \\approx d_{L \\to H} + d_{H \\to B}$ within tolerance δt\n",
    "- All synaptic weights must exceed $w_{\\min}$\n",
    "\n",
    "**Limitation**\n",
    "\n",
    "We only evaluate strictness (tightening δt from the baseline 3 ms). Relaxing δt beyond baseline could admit additional circuits not represented in the canonical merged results, which would require re-running detection before merging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timing tolerance strictness analysis\n",
    "dt_sensitivity_results = []\n",
    "\n",
    "for dt_thresh in DT_THRESHOLDS_MS:\n",
    "    # Create Constrained refinery with specified tolerance\n",
    "    filter_constrained = refinery.Constrained(syn_params, w_min=W_MIN, tol=dt_thresh)\n",
    "\n",
    "    for layer, polygrps in polygrps_by_layer.items():\n",
    "        # Apply stricter constraint\n",
    "        filtered = filter_constrained(polygrps)\n",
    "\n",
    "        dt_sensitivity_results.append({\n",
    "            'dt_tolerance_ms': dt_thresh,\n",
    "            'layer': layer,\n",
    "            'count': len(filtered),\n",
    "            'count_original': len(polygrps),\n",
    "            'retention_pct': 100 * len(filtered) / len(polygrps) if len(polygrps) > 0 else 0\n",
    "        })\n",
    "\n",
    "dt_df = pd.DataFrame(dt_sensitivity_results)\n",
    "print(\"Timing Tolerance Strictness Results:\")\n",
    "print(dt_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot table for cleaner display\n",
    "dt_pivot = dt_df.pivot(index='layer', columns='dt_tolerance_ms', values='count')\n",
    "dt_pivot.columns = [f'δt≤{c} ms' for c in dt_pivot.columns]\n",
    "dt_pivot.index = [f'L{i}' for i in dt_pivot.index]\n",
    "\n",
    "# Add totals\n",
    "dt_pivot.loc['Total'] = dt_pivot.sum()\n",
    "\n",
    "print(\"\\nHFB Counts by Timing Tolerance:\")\n",
    "print(dt_pivot)\n",
    "\n",
    "# Save to CSV\n",
    "dt_pivot.to_csv(OUTPUT_DIR / 'sensitivity_counts_by_dt.csv')\n",
    "print(f\"\\nSaved: {OUTPUT_DIR / 'sensitivity_counts_by_dt.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot: HFB counts vs timing tolerance\n",
    "fig, ax = plt.subplots(figsize=(5.5, 2.5))\n",
    "\n",
    "for layer in layers_to_analyse:\n",
    "    layer_data = dt_df[dt_df['layer'] == layer]\n",
    "    ax.plot(layer_data['dt_tolerance_ms'], layer_data['count'],\n",
    "            marker='o', label=f'L{layer}', linewidth=2, markersize=6)\n",
    "\n",
    "# Add total line\n",
    "total_counts = dt_df.groupby('dt_tolerance_ms')['count'].sum()\n",
    "ax.plot(total_counts.index, total_counts.values,\n",
    "        marker='s', label='Total', linewidth=1.5, markersize=6,\n",
    "        color='black', linestyle='--')\n",
    "\n",
    "ax.set_xlabel(r'Timing tolerance $\\delta t$ [ms]')\n",
    "ax.set_ylabel('# PNGs')\n",
    "ax.legend(loc='best')\n",
    "ax.grid(True, alpha=1)\n",
    "ax.set_xticks(DT_THRESHOLDS_MS)\n",
    "ax.set_ylim(None, 10000 * 1.11)\n",
    "\n",
    "fig.tight_layout()\n",
    "viz.save_figure(fig, OUTPUT_DIR / 'sensitivity_dt.pdf', overwrite=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Composition Analysis\n",
    "\n",
    "For each high-level neuron (H) in significant HFB circuits, count the number of distinct circuits that share that neuron. This provides evidence that triplet HFBs can form larger assemblies.\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "If high-level neurons participate in multiple distinct HFB circuits, it suggests that:\n",
    "1. The triplet motifs are not isolated structures\n",
    "2. Higher-order assemblies may emerge through shared neurons\n",
    "3. The network has developed compositional representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hfbs_per_high_neuron(polygrps: Sequence[PNG]) -> dict[int, int]:\n",
    "    \"\"\"Count distinct HFB circuits per high-level neuron.\n",
    "\n",
    "    The high-level neuron is the second-firing neuron (position 1 in the triplet).\n",
    "\n",
    "    Returns:\n",
    "        Dictionary mapping high-level neuron ID to count of distinct circuits.\n",
    "    \"\"\"\n",
    "    counts = defaultdict(int)\n",
    "    for png in polygrps:\n",
    "        # High-level neuron is at position 1 (second-firing)\n",
    "        high_nrn = png.nrns[1]\n",
    "        counts[high_nrn] += 1\n",
    "    return dict(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute composition metrics for each layer\n",
    "composition_results = []\n",
    "\n",
    "for layer, polygrps in polygrps_by_layer.items():\n",
    "    hfbs_per_high = compute_hfbs_per_high_neuron(polygrps)\n",
    "    counts = list(hfbs_per_high.values())\n",
    "\n",
    "    if counts:\n",
    "        composition_results.append({\n",
    "            'layer': layer,\n",
    "            'num_high_neurons': len(counts),\n",
    "            'total_hfbs': sum(counts),\n",
    "            'mean_hfbs_per_high': np.mean(counts),\n",
    "            'median_hfbs_per_high': np.median(counts),\n",
    "            'max_hfbs_per_high': np.max(counts),\n",
    "            'min_hfbs_per_high': np.min(counts),\n",
    "            'std_hfbs_per_high': np.std(counts)\n",
    "        })\n",
    "\n",
    "        print(f\"\\nLayer {layer}:\")\n",
    "        print(f\"  High-level neurons participating in HFBs: {len(counts)}\")\n",
    "        print(f\"  HFBs per high neuron: mean={np.mean(counts):.2f}, \"\n",
    "              f\"median={np.median(counts):.1f}, max={np.max(counts)}\")\n",
    "\n",
    "composition_df = pd.DataFrame(composition_results)\n",
    "composition_df.set_index('layer', inplace=True)\n",
    "composition_df.index = [f'L{i}' for i in composition_df.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display composition summary\n",
    "print(\"\\nComposition Summary:\")\n",
    "print(composition_df.round(2))\n",
    "\n",
    "# Save to CSV\n",
    "composition_df.to_csv(OUTPUT_DIR / 'composition_metrics.csv')\n",
    "print(f\"\\nSaved: {OUTPUT_DIR / 'composition_metrics.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of HFBs per high-level neuron (combined across layers)\n",
    "all_hfbs_per_high = []\n",
    "for layer, polygrps in polygrps_by_layer.items():\n",
    "    hfbs_per_high = compute_hfbs_per_high_neuron(polygrps)\n",
    "    all_hfbs_per_high.extend(hfbs_per_high.values())\n",
    "\n",
    "all_hfbs_per_high = np.array(all_hfbs_per_high)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 2.5))\n",
    "\n",
    "bins = np.arange(0.5, max(all_hfbs_per_high) + 1.5, 1)\n",
    "ax.hist(all_hfbs_per_high, bins=bins, edgecolor='black', alpha=1)\n",
    "ax.axvline(all_hfbs_per_high.mean(), color='C1', linestyle='--',\n",
    "           label=f'Mean ({all_hfbs_per_high.mean():.1f})')\n",
    "\n",
    "ax.set_xlim(0, None)\n",
    "\n",
    "ax.set_xlabel('# PNGs per high-level neuron')\n",
    "ax.set_ylabel('Count')  # 'Count (# high-level neurons)'\n",
    "ax.legend()\n",
    "ax.set_axisbelow(True)\n",
    "ax.grid(True, alpha=1)\n",
    "\n",
    "fig.tight_layout()\n",
    "viz.save_figure(fig, OUTPUT_DIR / 'composition_distribution.pdf', overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-layer composition distributions\n",
    "if len(layers_to_analyse) > 1:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(5.5, 3), sharex=True, sharey=True)\n",
    "    axes = axes.flatten()\n",
    "    ncols = 2\n",
    "\n",
    "    for idx, (ax, layer) in enumerate(zip(axes, layers_to_analyse)):\n",
    "        hfbs_per_high = compute_hfbs_per_high_neuron(polygrps_by_layer[layer])\n",
    "        counts = list(hfbs_per_high.values())\n",
    "\n",
    "        if counts:\n",
    "            mean_count = float(np.mean(counts))\n",
    "            bins = np.arange(0.5, max(counts) + 1.5, 1)\n",
    "            ax.hist(counts, bins=bins, edgecolor='black', linewidth=0.5, alpha=1)\n",
    "            ax.axvline(mean_count, color='C1', linestyle='--', linewidth=1.5,\n",
    "                       label=f'Mean ({mean_count:.1f})')\n",
    "            ax.set_title(f'Layer {layer}', fontsize=\"medium\", fontweight=\"bold\")\n",
    "            if idx >= ncols:\n",
    "                ax.set_xlabel('# PNGs per high neuron')\n",
    "            ax.legend()\n",
    "            ax.set_axisbelow(True)\n",
    "            ax.grid(True, alpha=1)\n",
    "\n",
    "    for ax in axes[len(layers_to_analyse):]:\n",
    "        ax.axis('off')\n",
    "\n",
    "    axes[0].set_xlim(0, 20)\n",
    "    axes[0].set_ylim(0, None)\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[2].set_ylabel('Count')\n",
    "    fig.tight_layout()\n",
    "    viz.save_figure(fig, OUTPUT_DIR / 'composition_distribution_by_layer.pdf', overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Composition analysis across span thresholds\n",
    "composition_by_span = []\n",
    "\n",
    "for span_thresh in SPAN_THRESHOLDS_MS:\n",
    "    filter_span = FilterBySpan(max_span=span_thresh)\n",
    "\n",
    "    for layer, polygrps in polygrps_by_layer.items():\n",
    "        filtered = filter_span(polygrps)\n",
    "        hfbs_per_high = compute_hfbs_per_high_neuron(filtered)\n",
    "        counts = list(hfbs_per_high.values())\n",
    "\n",
    "        if counts:\n",
    "            composition_by_span.append({\n",
    "                'span_threshold_ms': span_thresh,\n",
    "                'layer': layer,\n",
    "                'num_high_neurons': len(counts),\n",
    "                'mean_hfbs_per_high': np.mean(counts),\n",
    "                'max_hfbs_per_high': np.max(counts)\n",
    "            })\n",
    "\n",
    "composition_span_df = pd.DataFrame(composition_by_span)\n",
    "print(\"\\nComposition Metrics by Span Threshold:\")\n",
    "print(composition_span_df.round(2).to_string(index=False))\n",
    "\n",
    "# Save\n",
    "composition_span_df.to_csv(OUTPUT_DIR / 'composition_by_span.csv', index=False)\n",
    "print(f\"\\nSaved: {OUTPUT_DIR / 'composition_by_span.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary\n",
    "\n",
    "### Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary statistics\n",
    "print(\"=\" * 60)\n",
    "print(\"ROBUSTNESS & SENSITIVITY ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nExperiment: {EXPERIMENT_NAME}\")\n",
    "print(f\"Trial: {TRIAL_INDEX}\")\n",
    "print(f\"Layers analysed: {layers_to_analyse}\")\n",
    "print(f\"Total significant HFBs (baseline): {total_hfbs}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"1. WINDOW SENSITIVITY\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Span thresholds tested: {SPAN_THRESHOLDS_MS} ms\")\n",
    "print(f\"\\nRetention at smallest window ({SPAN_THRESHOLDS_MS[0]} ms):\")\n",
    "min_span = SPAN_THRESHOLDS_MS[0]\n",
    "min_span_total = span_df[span_df['span_threshold_ms'] == min_span]['count'].sum()\n",
    "print(f\"  {min_span_total}/{total_hfbs} ({100*min_span_total/total_hfbs:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"2. TIMING TOLERANCE STRICTNESS\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Tolerance thresholds tested: {DT_THRESHOLDS_MS} ms (baseline = {TOL_BASELINE} ms)\")\n",
    "print(f\"\\nRetention at strictest tolerance ({DT_THRESHOLDS_MS[0]} ms):\")\n",
    "min_dt = DT_THRESHOLDS_MS[0]\n",
    "min_dt_total = dt_df[dt_df['dt_tolerance_ms'] == min_dt]['count'].sum()\n",
    "print(f\"  {min_dt_total}/{total_hfbs} ({100*min_dt_total/total_hfbs:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"3. COMPOSITION ANALYSIS\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"High-level neurons participating in HFBs: {len(all_hfbs_per_high)}\")\n",
    "print(f\"Mean HFBs per high neuron: {all_hfbs_per_high.mean():.2f}\")\n",
    "print(f\"Max HFBs per high neuron: {all_hfbs_per_high.max()}\")\n",
    "multi_hfb_neurons = np.sum(all_hfbs_per_high > 1)\n",
    "print(f\"Neurons with >1 HFB: {multi_hfb_neurons} ({100*multi_hfb_neurons/len(all_hfbs_per_high):.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Results saved to: {OUTPUT_DIR}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save summary to text file\n",
    "summary_path = OUTPUT_DIR / 'analysis_summary.txt'\n",
    "with open(summary_path, 'w') as f:\n",
    "    f.write(\"ROBUSTNESS & SENSITIVITY ANALYSIS SUMMARY\\n\")\n",
    "    f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "    f.write(f\"Experiment: {EXPERIMENT_NAME}\\n\")\n",
    "    f.write(f\"Trial: {TRIAL_INDEX}\\n\")\n",
    "    f.write(f\"Layers analysed: {layers_to_analyse}\\n\")\n",
    "    f.write(f\"Total significant HFBs (baseline): {total_hfbs}\\n\\n\")\n",
    "\n",
    "    f.write(\"WINDOW SENSITIVITY\\n\")\n",
    "    f.write(\"-\" * 30 + \"\\n\")\n",
    "    f.write(span_pivot.to_string() + \"\\n\\n\")\n",
    "\n",
    "    f.write(\"TIMING TOLERANCE STRICTNESS\\n\")\n",
    "    f.write(\"-\" * 30 + \"\\n\")\n",
    "    f.write(dt_pivot.to_string() + \"\\n\\n\")\n",
    "\n",
    "    f.write(\"COMPOSITION METRICS\\n\")\n",
    "    f.write(\"-\" * 30 + \"\\n\")\n",
    "    f.write(composition_df.to_string() + \"\\n\")\n",
    "\n",
    "print(f\"Summary saved to: {summary_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hsnn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
